\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{bertsimas2015best}
\citation{kim2009ell_1}
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear Models for the Quantile Autoregression}{5}{section.2}}
\newlabel{sec:linear-models}{{2}{5}{Linear Models for the Quantile Autoregression}{section.2}{}}
\newlabel{eq:qar-lp}{{2.2}{5}{Linear Models for the Quantile Autoregression}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Best subset selection with Mixed Integer Programming}{5}{subsection.2.1}}
\newlabel{sec:best-subset-mip}{{2.1}{5}{Best subset selection with Mixed Integer Programming}{subsection.2.1}{}}
\newlabel{linear1}{{2.4}{5}{Best subset selection with Mixed Integer Programming}{equation.2.3}{}}
\newlabel{linear2}{{2.5}{5}{Best subset selection with Mixed Integer Programming}{equation.2.3}{}}
\newlabel{linear3}{{2.6}{5}{Best subset selection with Mixed Integer Programming}{equation.2.3}{}}
\newlabel{linear4}{{2.7}{5}{Best subset selection with Mixed Integer Programming}{equation.2.3}{}}
\newlabel{eq:linear5}{{2.8}{5}{Best subset selection with Mixed Integer Programming}{equation.2.3}{}}
\citation{li2012l1}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Linear Quantile Regression when only $y_{t-1}$ is used as explanatory variable\relax }}{6}{figure.caption.4}}
\newlabel{fig:icaraizinho-crossing-200}{{2.1}{6}{Linear Quantile Regression when only $y_{t-1}$ is used as explanatory variable\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Best subset selection with a $\ell _1$ penalty}{6}{subsection.2.2}}
\newlabel{sec:best-subset-ell1}{{2.2}{6}{Best subset selection with a $\ell _1$ penalty}{subsection.2.2}{}}
\newlabel{eq:l1-qar-optim}{{2.9}{6}{Best subset selection with a $\ell _1$ penalty}{equation.2.9}{}}
\newlabel{eq:obj-lasso}{{2.10}{6}{Best subset selection with a $\ell _1$ penalty}{equation.2.10}{}}
\newlabel{l1-qar-3}{{2.13}{6}{Best subset selection with a $\ell _1$ penalty}{equation.2.10}{}}
\newlabel{l1-qar-4}{{2.14}{6}{Best subset selection with a $\ell _1$ penalty}{equation.2.10}{}}
\citation{belloni2009least}
\newlabel{eq:post-lasso}{{2.15}{7}{Best subset selection with a $\ell _1$ penalty}{equation.2.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Simulation Study}{7}{subsection.2.3}}
\newlabel{sec:simulation-ar1}{{2.3}{7}{Simulation Study}{subsection.2.3}{}}
\newlabel{eq:sim-true-model}{{2.16}{7}{Simulation Study}{equation.2.16}{}}
\newlabel{fig:npqar-cross}{{\caption@xref {fig:npqar-cross}{ on input line 99}}{8}{Best subset selection with a $\ell _1$ penalty}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Coefficients path for a few different values of $\alpha $-quantiles. $\lambda $ is presented in a $\qopname  \relax o{log}_{10}$ scale, to make visualization easier.\relax }}{8}{figure.caption.5}}
\newlabel{fig:npqar-results}{{2.2}{8}{Coefficients path for a few different values of $\alpha $-quantiles. $\lambda $ is presented in a $\log _{10}$ scale, to make visualization easier.\relax }{figure.caption.5}{}}
\citation{machado1993robust}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Model selection}{9}{subsection.2.4}}
\newlabel{eq:SIC}{{2.22}{10}{Model selection}{equation.2.22}{}}
\newlabel{eq:distance}{{2.25}{10}{Model selection}{equation.2.25}{}}
\newlabel{fig:npqar-cross}{{\caption@xref {fig:npqar-cross}{ on input line 228}}{11}{Model selection}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison of SIC between a solution with Lasso as a variable selector and the best subset selection with MIP. The bars represent the distance $d$ as defined by equation \ref  {eq:distance}.   (*) When the distance is zero, it means that the same variables are selected from both methods for a given $k$. Thus, in these cases we have the same SIC for both of them.\relax }}{11}{figure.caption.6}}
\newlabel{fig:comparison-lm-results}{{2.3}{11}{Comparison of SIC between a solution with Lasso as a variable selector and the best subset selection with MIP. The bars represent the distance $d$ as defined by equation \ref {eq:distance}. \\ (*) When the distance is zero, it means that the same variables are selected from both methods for a given $k$. Thus, in these cases we have the same SIC for both of them.\relax }{figure.caption.6}{}}
\@setckpt{tnen-linear-models}{
\setcounter{page}{12}
\setcounter{equation}{25}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{3}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{6}
\setcounter{float@type}{4}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{section@level}{2}
}
