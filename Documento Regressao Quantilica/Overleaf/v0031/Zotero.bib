
@article{price_double-time_1998,
	title = {double-time {Is} a {Novel} {Drosophila} {Clock} {Gene} that {Regulates} {PERIOD} {Protein} {Accumulation}},
	volume = {94},
	issn = {0092-8674},
	url = {http://www.sciencedirect.com/science/article/pii/S0092867400812246},
	doi = {10.1016/S0092-8674(00)81224-6},
	abstract = {We have isolated three alleles of a novel Drosophila clock gene, double-time (dbt). Short- (dbtS) and long-period (dbtL) mutants alter both behavioral rhythmicity and molecular oscillations from previously identified clock genes, period and timeless. A third allele, dbtP, causes pupal lethality and eliminates circadian cycling of per and tim gene products in larvae. In dbtP mutants, PER proteins constitutively accumulate, remain hypophosphorylated, and no longer depend on TIM proteins for their accumulation. We propose that the normal function of DOUBLETIME protein is to reduce the stability and thus the level of accumulation of monomeric PER proteins. This would promote a delay between per/tim transcription and PER/TIM complex function, which is essential for molecular rhythmicity.},
	number = {1},
	urldate = {2017-09-28TZ},
	journal = {Cell},
	author = {Price, Jeffrey L and Blau, Justin and Rothenfluh, Adrian and Abodeely, Marla and Kloss, Brian and Young, Michael W},
	month = jul,
	year = {1998},
	pages = {83--95}
}

@article{hong_probabilistic_2016,
	title = {Probabilistic energy forecasting: {Global} {Energy} {Forecasting} {Competition} 2014 and beyond},
	volume = {32},
	issn = {0169-2070},
	shorttitle = {Probabilistic energy forecasting},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207016000133},
	doi = {10.1016/j.ijforecast.2016.02.001},
	abstract = {The energy industry has been going through a significant modernization process over the last decade. Its infrastructure is being upgraded rapidly. The supply, demand and prices are becoming more volatile and less predictable than ever before. Even its business model is being challenged fundamentally. In this competitive and dynamic environment, many decision-making processes rely on probabilistic forecasts to quantify the uncertain future. Although most of the papers in the energy forecasting literature focus on point or single-valued forecasts, the research interest in probabilistic energy forecasting research has taken off rapidly in recent years. In this paper, we summarize the recent research progress on probabilistic energy forecasting. A major portion of the paper is devoted to introducing the Global Energy Forecasting Competition 2014 (GEFCom2014), a probabilistic energy forecasting competition with four tracks on load, price, wind and solar forecasting, which attracted 581 participants from 61 countries. We conclude the paper with 12 predictions for the next decade of energy forecasting.},
	number = {3},
	urldate = {2017-09-18TZ},
	journal = {International Journal of Forecasting},
	author = {Hong, Tao and Pinson, Pierre and Fan, Shu and Zareipour, Hamidreza and Troccoli, Alberto and Hyndman, Rob J.},
	month = jul,
	year = {2016},
	keywords = {Electric load forecasting, Electricity price forecasting, Forecasting competition, Probabilistic forecasting, Solar power forecasting, Wind power forecasting},
	pages = {896--913}
}

@article{hong_probabilistic_2016-1,
	title = {Probabilistic energy forecasting: {Global} {Energy} {Forecasting} {Competition} 2014 and beyond},
	volume = {32},
	issn = {0169-2070},
	shorttitle = {Probabilistic energy forecasting},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207016000133},
	doi = {10.1016/j.ijforecast.2016.02.001},
	abstract = {The energy industry has been going through a significant modernization process over the last decade. Its infrastructure is being upgraded rapidly. The supply, demand and prices are becoming more volatile and less predictable than ever before. Even its business model is being challenged fundamentally. In this competitive and dynamic environment, many decision-making processes rely on probabilistic forecasts to quantify the uncertain future. Although most of the papers in the energy forecasting literature focus on point or single-valued forecasts, the research interest in probabilistic energy forecasting research has taken off rapidly in recent years. In this paper, we summarize the recent research progress on probabilistic energy forecasting. A major portion of the paper is devoted to introducing the Global Energy Forecasting Competition 2014 (GEFCom2014), a probabilistic energy forecasting competition with four tracks on load, price, wind and solar forecasting, which attracted 581 participants from 61 countries. We conclude the paper with 12 predictions for the next decade of energy forecasting.},
	number = {3},
	urldate = {2017-09-18TZ},
	journal = {International Journal of Forecasting},
	author = {Hong, Tao and Pinson, Pierre and Fan, Shu and Zareipour, Hamidreza and Troccoli, Alberto and Hyndman, Rob J.},
	month = jul,
	year = {2016},
	keywords = {Electric load forecasting, Electricity price forecasting, Forecasting competition, Probabilistic forecasting, Solar power forecasting, Wind power forecasting},
	pages = {896--913}
}

@techreport{bergmeir_note_2017,
	title = {A {Note} on the {Validity} of {Cross}-{Validation} for {Evaluating} {Time} {Series} {Prediction}},
	url = {https://ideas.repec.org/p/msh/ebswps/2015-10.html},
	abstract = {One of the most widely used standard procedures for model evaluation in classification and regression is K-fold cross-validation (CV). However, when it comes to time series forecasting, because of the inherent serial correlation and potential non-stationarity of the data, its application is not straightforward and often omitted by practitioners in favor of an out-of-sample (OOS) evaluation. In this paper, we show that the particular setup in which time series forecasting is usually performed using Machine Learning methods renders the use of standard K-fold CV possible. We present theoretical insights supporting our arguments. Furthermore, we present a simulation study where we show empirically that K-fold CV performs favourably compared to both OOS evaluation and other time-series-specific techniques such as non-dependent cross-validation.},
	number = {10/15},
	urldate = {2017-09-17TZ},
	institution = {Monash University, Department of Econometrics and Business Statistics},
	author = {Bergmeir, Christoph and Hyndman, Rob J. and Koo, Bonsoo},
	year = {2017},
	keywords = {Cross-validation, Time series, auto regression.}
}

@article{hocking_selection_1967,
	title = {Selection of the {Best} {Subset} in {Regression} {Analysis}},
	volume = {9},
	issn = {0040-1706},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1967.10490502},
	doi = {10.1080/00401706.1967.10490502},
	abstract = {The problem of selecting the best subset or subsets of independent variables in a multiple linear regression analysis is two-fold. The first, and most important problem is the development of criterion for choosing between two contending subsets. Applying these criteria to all possible subsets, if the number of independent variables is large, may not be economically feasible and so the second problem is concerned with decreasing the computational effort. This paper is concerned with the second question using the C p -statistic of Mallows as the basic criterion for comparing two regressions. A procedure is developed which will indicate ‘good’ regressions with B minimum of computation.},
	number = {4},
	urldate = {2017-09-14TZ},
	journal = {Technometrics},
	author = {Hocking, R. R. and Leslie, R. N.},
	month = nov,
	year = {1967},
	pages = {531--540}
}

@article{hocking_selection_1967-1,
	title = {Selection of the {Best} {Subset} in {Regression} {Analysis}},
	volume = {9},
	issn = {0040-1706},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1967.10490502},
	doi = {10.1080/00401706.1967.10490502},
	abstract = {The problem of selecting the best subset or subsets of independent variables in a multiple linear regression analysis is two-fold. The first, and most important problem is the development of criterion for choosing between two contending subsets. Applying these criteria to all possible subsets, if the number of independent variables is large, may not be economically feasible and so the second problem is concerned with decreasing the computational effort. This paper is concerned with the second question using the C p -statistic of Mallows as the basic criterion for comparing two regressions. A procedure is developed which will indicate ‘good’ regressions with B minimum of computation.},
	number = {4},
	urldate = {2017-09-14TZ},
	journal = {Technometrics},
	author = {Hocking, R. R. and Leslie, R. N.},
	month = nov,
	year = {1967},
	pages = {531--540}
}

@misc{andale_conditional_nodate,
	title = {Conditional {Distribution}: {Definition} and {Examples}},
	shorttitle = {Conditional {Distribution}},
	url = {http://www.statisticshowto.com/conditional-distribution/},
	abstract = {Conditional Distribution: Definition and Examples was last modified: March 23rd, 2017 by},
	urldate = {2017-09-11TZ},
	journal = {Statistics How To},
	author = {Andale}
}

@article{taylor_forecasting_2015,
	title = {Forecasting wind power quantiles using conditional kernel estimation},
	volume = {80},
	issn = {0960-1481},
	url = {http://www.sciencedirect.com/science/article/pii/S0960148115001123},
	doi = {10.1016/j.renene.2015.02.022},
	abstract = {The efficient management of wind farms and electricity systems benefit greatly from accurate wind power quantile forecasts. For example, when a wind power producer offers power to the market for a future period, the optimal bid is a quantile of the wind power density. An approach based on conditional kernel density (CKD) estimation has previously been used to produce wind power density forecasts. The approach is appealing because: it makes no distributional assumption for wind power; it captures the uncertainty in forecasts of wind velocity; it imposes no assumption for the relationship between wind power and wind velocity; and it allows more weight to be put on more recent observations. In this paper, we adapt this approach. As we do not require an estimate of the entire wind power density, our new proposal is to optimise the CKD-based approach specifically towards estimation of the desired quantile, using the quantile regression objective function. Using data from three European wind farms, we obtained encouraging results for this new approach. We also achieved good results with a previously proposed method of constructing a wind power quantile as the sum of a point forecast and a forecast error quantile estimated using quantile regression.},
	urldate = {2017-08-31TZ},
	journal = {Renewable Energy},
	author = {Taylor, James W. and Jeon, Jooyoung},
	month = aug,
	year = {2015},
	keywords = {Conditional kernel estimation, Quantiles, quantile regression, wind power},
	pages = {370--379}
}

@article{taylor_forecasting_2015-1,
	title = {Forecasting wind power quantiles using conditional kernel estimation},
	volume = {80},
	issn = {0960-1481},
	url = {http://www.sciencedirect.com/science/article/pii/S0960148115001123},
	doi = {10.1016/j.renene.2015.02.022},
	abstract = {The efficient management of wind farms and electricity systems benefit greatly from accurate wind power quantile forecasts. For example, when a wind power producer offers power to the market for a future period, the optimal bid is a quantile of the wind power density. An approach based on conditional kernel density (CKD) estimation has previously been used to produce wind power density forecasts. The approach is appealing because: it makes no distributional assumption for wind power; it captures the uncertainty in forecasts of wind velocity; it imposes no assumption for the relationship between wind power and wind velocity; and it allows more weight to be put on more recent observations. In this paper, we adapt this approach. As we do not require an estimate of the entire wind power density, our new proposal is to optimise the CKD-based approach specifically towards estimation of the desired quantile, using the quantile regression objective function. Using data from three European wind farms, we obtained encouraging results for this new approach. We also achieved good results with a previously proposed method of constructing a wind power quantile as the sum of a point forecast and a forecast error quantile estimated using quantile regression.},
	urldate = {2017-08-31TZ},
	journal = {Renewable Energy},
	author = {Taylor, James W. and Jeon, Jooyoung},
	month = aug,
	year = {2015},
	keywords = {Conditional kernel estimation, Quantiles, quantile regression, wind power},
	pages = {370--379}
}

@article{bergmeir_use_2012,
	series = {Data {Mining} for {Software} {Trustworthiness}},
	title = {On the use of cross-validation for time series predictor evaluation},
	volume = {191},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025511006773},
	doi = {10.1016/j.ins.2011.12.028},
	abstract = {In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand. In traditional forecasting, it is common practice to reserve a part from the end of each time series for testing, and to use the rest of the series for training. Thus it is not made full use of the data, but theoretical problems with respect to temporal evolutionary effects and dependencies within the data as well as practical problems regarding missing values are eliminated. On the other hand, when evaluating machine learning and other regression methods used for time series forecasting, often cross-validation is used for evaluation, paying little attention to the fact that those theoretical problems invalidate the fundamental assumptions of cross-validation. To close this gap and examine the consequences of different model selection procedures in practice, we have developed a rigorous and extensive empirical study. Six different model selection procedures, based on (i) cross-validation and (ii) evaluation using the series’ last part, are used to assess the performance of four machine learning and other regression techniques on synthetic and real-world time series. No practical consequences of the theoretical flaws were found during our study, but the use of cross-validation techniques led to a more robust model selection. To make use of the “best of both worlds”, we suggest that the use of a blocked form of cross-validation for time series evaluation became the standard procedure, thus using all available information and circumventing the theoretical problems.},
	urldate = {2017-08-28TZ},
	journal = {Information Sciences},
	author = {Bergmeir, Christoph and Benítez, José M.},
	month = may,
	year = {2012},
	keywords = {Cross-validation, Error measures, Machine learning, Predictor evaluation, Time series, regression},
	pages = {192--213}
}

@article{moeanaddin_numerical_1990,
	title = {Numerical {Evaluation} of {Distributions} in {Non}-{Linear} {Autoregression}},
	volume = {11},
	issn = {1467-9892},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9892.1990.tb00040.x/abstract},
	doi = {10.1111/j.1467-9892.1990.tb00040.x},
	abstract = {Abstract.  We use the Chapman-Kolmogorov formula as a recursive relation for computing the m-step-ahead conditional density of a non-linear autoregressive model. We approximate the stationary marginal probability density function of the model by the m-step-ahead conditional density for sufficiently large m. An advantage of our method is its simple implementation; only one NAG subroutine is needed. We have also studied the advantage of incorporating the matrix-squaring procedure.},
	language = {en},
	number = {1},
	urldate = {2017-07-06TZ},
	journal = {Journal of Time Series Analysis},
	author = {Moeanaddin, R. and Tong, Howell},
	month = jan,
	year = {1990},
	keywords = {Chapman-Kolmogorov formula, EXPAR models, Gauss-type formula, SETAR models, conditional density, eigenvalues, integral equation, matrix squaring, non-linear autoregressive models, numerical integration},
	pages = {33--48}
}

@article{brown_time_1984,
	title = {Time {Series} {Models} to {Simulate} and {Forecast} {Wind} {Speed} and {Wind} {Power}},
	volume = {23},
	issn = {0733-3021},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0450%281984%29023%3C1184%3ATSMTSA%3E2.0.CO%3B2},
	doi = {10.1175/1520-0450(1984)023<1184:TSMTSA>2.0.CO;2},
	abstract = {A general approach for modeling wind speed and wind power is described. Because wind power is a function of wind speed, the methodology is based on the development of a model of wind speed. Values of wind power are estimated by applying the appropriate transformations to values of wind speed. The wind speed modeling approach takes into account several basic features of wind speed data, including autocorrelation, non-Gaussian distribution, and diurnal nonstationarity. The positive correlation between consecutive wind speed observations is taken into account by fitting an autoregressive process to wind speed data transformed to make their distribution approximately Gaussian and standardized to remove diurnal nonstationarity. As an example, the modeling approach is applied to a small set of hourly wind speed data from the Pacific Northwest. Use of the methodology for simulating and forecasting wind speed and wind power is discussed and an illustration of each of these types of applications is presented. To take into account the uncertainty of wind speed and wind power forecasts, techniques are presented for expressing the forecasts either in terms of confidence intervals or in terms of probabilities.},
	number = {8},
	urldate = {2017-07-06TZ},
	journal = {Journal of Climate and Applied Meteorology},
	author = {Brown, Barbara G. and Katz, Richard W. and Murphy, Allan H.},
	month = aug,
	year = {1984},
	pages = {1184--1195}
}

@article{bludszuweit_statistical_2008,
	title = {Statistical {Analysis} of {Wind} {Power} {Forecast} {Error}},
	volume = {23},
	issn = {0885-8950},
	doi = {10.1109/TPWRS.2008.922526},
	abstract = {Wind power forecast error usually has been assumed to have a near Gaussian distribution. With a simple statistical analysis, it can be shown that this is not valid. To obtain a more appropriate probability density function (pdf) of the wind power forecast error, an indirect algorithm based on the Beta pdf is proposed. Measured one-year time series from two different wind farms are used to generate the forecast data. Three different forecast scenarios are simulated based on the persistence approach. This makes the results comparable to other forecast methods. It is found that the forecast error pdf has a variable kurtosis ranging from 3 (like the Gaussian) to over 10, and therefore it can be categorized as fat-tailed. A new approximation function for the parameters of the Beta pdf is proposed because results from former publications could not be confirmed. Besides, a linear approximation is developed to describe the relationship between the persistence forecast and the related mean measured power. An energy storage system (ESS), which reduces the forecast error and smooths the wind power output, is considered. Results for this case show the usefulness of the proposed forecast error pdf for finding the optimum rated ESS power.},
	number = {3},
	journal = {IEEE Transactions on Power Systems},
	author = {Bludszuweit, H. and Dominguez-Navarro, J. A. and Llombart, A.},
	month = aug,
	year = {2008},
	keywords = {Beta pdf, ESS, Error analysis, Forecasting, Gaussian distribution, Wind power generation, approximation function, energy storage system, load forecasting, probability, probability density function, statistical analysis, wind, wind farms, wind power forecast error, wind power plants},
	pages = {983--991}
}

@article{bremnes_comparison_2006,
	title = {A comparison of a few statistical models for making quantile wind power forecasts},
	volume = {9},
	issn = {1099-1824},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/we.182/abstract},
	doi = {10.1002/we.182},
	abstract = {During the last few years, probabilistic wind power forecasts have received increasing attention because of their assumed value in decision-making processes. In the current article, three statistical methods are described and several models based on these are compared. The statistical methods are local quantile regression, a local Gaussian model and the Nadaraya–Watson estimator for conditional cumulative distribution functions. The focus is on quantile forecasts, since these often provide the required type of information to make optimal economic decisions and are ideal for visualizing uncertainty. The statistical methods are applied to data from a wind farm in Norway and results are compared using appropriate measures for assessment of quantile forecasts and in terms of a simple model for economic value. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1-2},
	urldate = {2017-06-28TZ},
	journal = {Wind Energy},
	author = {Bremnes, John Bjørnar},
	month = jan,
	year = {2006},
	keywords = {Nadaraya–Watson estimator, economic value, local Gaussian model, probabilistic forecasts, quantile forecasts, quantile regression, wind power},
	pages = {3--11}
}

@article{bremnes_comparison_2006-1,
	title = {A comparison of a few statistical models for making quantile wind power forecasts},
	volume = {9},
	issn = {1099-1824},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/we.182/abstract},
	doi = {10.1002/we.182},
	abstract = {During the last few years, probabilistic wind power forecasts have received increasing attention because of their assumed value in decision-making processes. In the current article, three statistical methods are described and several models based on these are compared. The statistical methods are local quantile regression, a local Gaussian model and the Nadaraya–Watson estimator for conditional cumulative distribution functions. The focus is on quantile forecasts, since these often provide the required type of information to make optimal economic decisions and are ideal for visualizing uncertainty. The statistical methods are applied to data from a wind farm in Norway and results are compared using appropriate measures for assessment of quantile forecasts and in terms of a simple model for economic value. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1-2},
	urldate = {2017-06-28TZ},
	journal = {Wind Energy},
	author = {Bremnes, John Bjørnar},
	month = jan,
	year = {2006},
	keywords = {Nadaraya–Watson estimator, economic value, local Gaussian model, probabilistic forecasts, quantile forecasts, quantile regression, wind power},
	pages = {3--11}
}

@article{koenker_quantile_2006,
	title = {Quantile {Autoregression} [with {Comments}, {Rejoinder}]},
	volume = {101},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/27590777},
	abstract = {We consider quantile autoregression (QAR) models in which the autoregressive coefficients can be expressed as monotone functions of a single, scalar random variable. The models can capture systematic influences of conditioning variables on the location, scale, and shape of the conditional distribution of the response, and thus constitute a significant extension of classical constant coefficient linear time series models in which the effect of conditioning is confined to a location shift. The models may be interpreted as a special case of the general random-coefficient autoregression model with strongly dependent coefficients. Statistical properties of the proposed model and associated estimators are studied. The limiting distributions of the autoregression quantile process are derived. QAR inference methods are also investigated. Empirical applications of the model to the U.S. unemployment rate, short-term interest rate, and gasoline prices highlight the model's potential.},
	number = {475},
	urldate = {2017-06-27TZ},
	journal = {Journal of the American Statistical Association},
	author = {Koenker, Roger and Xiao, Zhijie and Fan, Jianqing and Fan, Yingying and Knight, Marc and Hallin, Marc and Werker, Bas J. M. and Hafner, Christian M. and Linton, Oliver B. and Robinson, P. M.},
	year = {2006},
	pages = {980--1006}
}

@article{koenker_quantile_2006-1,
	title = {Quantile {Autoregression} [with {Comments}, {Rejoinder}]},
	volume = {101},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/27590777},
	abstract = {We consider quantile autoregression (QAR) models in which the autoregressive coefficients can be expressed as monotone functions of a single, scalar random variable. The models can capture systematic influences of conditioning variables on the location, scale, and shape of the conditional distribution of the response, and thus constitute a significant extension of classical constant coefficient linear time series models in which the effect of conditioning is confined to a location shift. The models may be interpreted as a special case of the general random-coefficient autoregression model with strongly dependent coefficients. Statistical properties of the proposed model and associated estimators are studied. The limiting distributions of the autoregression quantile process are derived. QAR inference methods are also investigated. Empirical applications of the model to the U.S. unemployment rate, short-term interest rate, and gasoline prices highlight the model's potential.},
	number = {475},
	urldate = {2017-06-27TZ},
	journal = {Journal of the American Statistical Association},
	author = {Koenker, Roger and Xiao, Zhijie and Fan, Jianqing and Fan, Yingying and Knight, Marc and Hallin, Marc and Werker, Bas J. M. and Hafner, Christian M. and Linton, Oliver B. and Robinson, P. M.},
	year = {2006},
	pages = {980--1006}
}

@article{cai_general_2016,
	title = {A {General} {Quantile} {Function} {Model} for {Economic} and {Financial} {Time} {Series}},
	volume = {35},
	issn = {0747-4938},
	url = {http://dx.doi.org/10.1080/07474938.2014.976528},
	doi = {10.1080/07474938.2014.976528},
	abstract = {This article proposed a general quantile function model that covers both one- and multiple-dimensional models and that takes several existing models in the literature as its special cases. This article also developed a new uniform Bayesian framework for quantile function modelling and illustrated the developed approach through different quantile function models. Many distributions are defined explicitly only via their quanitle functions as the corresponding distribution or density functions do not have an explicit mathematical expression. Such distributions are rarely used in economic and financial modelling in practice. The developed methodology makes it more convenient to use these distributions in analyzing economic and financial data. Empirical applications to economic and financial time series and comparisons with other types of models and methods show that the developed method can be very useful in practice.},
	number = {7},
	urldate = {2017-06-26TZ},
	journal = {Econometric Reviews},
	author = {Cai, Yuzhi},
	month = aug,
	year = {2016},
	keywords = {Bayesian approach, C1, C11, C22, Currency exchange rates, German DAX, Quantile functions models},
	pages = {1173--1193}
}

@article{cai_autoregression_2009,
	title = {Autoregression with {Non}-{Gaussian} {Innovations}},
	volume = {1},
	issn = {1941-1928},
	url = {https://www.degruyter.com/view/j/jtse.2009.1.2/jtse.2009.1.2.1016/jtse.2009.1.2.1016.xml},
	doi = {10.2202/1941-1928.1016},
	abstract = {Many economics and finance time series are non-Gaussian. In this paper, we propose a Bayesian approach to non-Gaussian autoregressive time series models via quantile functions. This approach is parametric, so we also compare the proposed parametric approach with a semi-parametric approach. Simulation studies and applications to real time series show that this method works very well.},
	number = {2},
	urldate = {2017-06-26TZ},
	journal = {Journal of Time Series Econometrics},
	author = {Cai, Yuzhi},
	year = {2009}
}

@article{li_arma_1988,
	title = {Arma {Modelling} with {Non}-{Gaussian} {Innovations}},
	volume = {9},
	issn = {1467-9892},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9892.1988.tb00461.x/abstract},
	doi = {10.1111/j.1467-9892.1988.tb00461.x},
	abstract = {Abstract.  The problem of modelling time series driven by non-Gaussian innovations is considered. The asymptotic normality of the maximum likelihood estimator is established under some general conditions. The distribution of the residual autocorrelations is also obtained. This gives rise to a potentially useful goodness-of-fit statistic. Applications of the results to two important cases are discussed. Two real examples are considered.},
	language = {en},
	number = {2},
	urldate = {2017-06-26TZ},
	journal = {Journal of Time Series Analysis},
	author = {Li, W. K. and McLeod, A. I.},
	month = mar,
	year = {1988},
	keywords = {Autoregressive moving-average process, maximum likelihood estimation, non-Gaussian innovations, residual autocorrelations},
	pages = {155--168}
}

@article{ciuperca_adaptive_2016,
	title = {Adaptive {LASSO} model selection in a multiphase quantile regression},
	volume = {50},
	issn = {0233-1888},
	url = {http://dx.doi.org/10.1080/02331888.2016.1151427},
	doi = {10.1080/02331888.2016.1151427},
	abstract = {We propose a general adaptive LASSO method for a quantile regression model. Our method is very interesting when we know nothing about the first two moments of the model error. We first prove that the obtained estimators satisfy the oracle properties, which involves the relevant variable selection without using hypothesis test. Next, we study the proposed method when the (multiphase) model changes to unknown observations called change-points. Convergence rates of the change-points and of the regression parameter estimators in each phase are found. The sparsity of the adaptive LASSO quantile estimators of the regression parameters is not affected by the change-points estimation. If the number of phases is unknown, a consistent criterion is proposed. Numerical studies by Monte Carlo simulations show the performance of the proposed method, compared to other existing methods in the literature, for models with a single phase or for multiphase models. The adaptive LASSO quantile method performs better than known variable selection methods, as the least squared method with adaptive LASSO penalty, L1-method with LASSO-type penalty and quantile method with SCAD penalty.},
	number = {5},
	urldate = {2017-06-20TZ},
	journal = {Statistics},
	author = {Ciuperca, Gabriela},
	month = sep,
	year = {2016},
	pages = {1100--1131}
}

@misc{noauthor_[1309.1262]_nodate,
	title = {[1309.1262] {Adaptive} {LASSO} model selection in a multiphase quantile regression},
	url = {https://arxiv.org/abs/1309.1262},
	urldate = {2017-06-20TZ}
}

@article{wan_direct_2017,
	title = {Direct {Quantile} {Regression} for {Nonparametric} {Probabilistic} {Forecasting} of {Wind} {Power} {Generation}},
	volume = {32},
	issn = {0885-8950},
	doi = {10.1109/TPWRS.2016.2625101},
	abstract = {The fluctuation and uncertainty of wind power generation bring severe challenges to secure and economic operation of power systems. Because wind power forecasting error is unavoidable, probabilistic forecasting becomes critical to accurately quantifying the uncertainty involved in traditional point forecasts of wind power and to providing meaningful information to conduct risk management in power system operation. This paper proposes a novel direct quantile regression approach to efficiently generate nonparametric probabilistic forecasting of wind power generation combining extreme learning machine and quantile regression. Quantiles with different proportions can be directly produced via an innovatively formulated linear programming optimization model, without dependency on point forecasts. Multistep probabilistic forecasting of 10-min wind power is newly carried out based on real wind farm data from Bornholm Island in Denmark. The superiority of the proposed approach is verified through comparisons with other well-established benchmarks. The proposed approach forms a new artificial neural network-based nonparametric forecasting framework for wind power with high efficiency, reliability, and flexibility, which can be beneficial to various decision-making activities in power systems.},
	number = {4},
	journal = {IEEE Transactions on Power Systems},
	author = {Wan, C. and Lin, J. and Wang, J. and Song, Y. and Dong, Z. Y.},
	month = jul,
	year = {2017},
	keywords = {Extreme learning machine, Forecasting, Predictive models, Probabilistic forecasting, Probabilistic logic, Reliability, Uncertainty, Wind forecasting, Wind power generation, linear programming, quantile regression, wind power},
	pages = {2767--2778}
}

@article{takeuchi_nonparametric_2006,
	title = {Nonparametric {Quantile} {Estimation}},
	volume = {7},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1248547.1248592},
	abstract = {In regression, the desired estimate of y{\textbar}x is not always given by a conditional mean, although this is most common. Sometimes one wants to obtain a good estimate that satisfies the property that a proportion, τ, of y{\textbar}x, will be below the estimate. For τ = 0.5 this is an estimate of the median. What might be called median regression, is subsumed under the term quantile regression. We present a nonparametric version of a quantile estimator, which can be obtained by solving a simple quadratic programming problem and provide uniform convergence statements and bounds on the quantile property of our estimator. Experimental results show the feasibility of the approach and competitiveness of our method with existing ones. We discuss several types of extensions including an approach to solve the quantile crossing problems, as well as a method to incorporate prior qualitative knowledge such as monotonicity constraints.},
	urldate = {2017-06-19TZ},
	journal = {J. Mach. Learn. Res.},
	author = {Takeuchi, Ichiro and Le, Quoc V. and Sears, Timothy D. and Smola, Alexander J.},
	month = dec,
	year = {2006},
	pages = {1231--1264}
}

@article{enss_nonparametric_2016,
	title = {Nonparametric {Quantile} {Estimation} {Based} on {Surrogate} {Models}},
	volume = {62},
	issn = {0018-9448},
	doi = {10.1109/TIT.2016.2586080},
	abstract = {Nonparametric estimation of a quantile qm(X),α of a random variable m(X) is considered, where m : ℝd → ℝ is a function, which is costly to compute and X is an ℝd-valued random variable with known distribution. Monte Carlo surrogate quantile estimates are considered, where in a first step, the function m is estimated by some estimate (surrogate) mn, and then, the quantile qm(X),α is estimated by a Monte Carlo estimate of the quantile qmn(X),α. A general error bound on the error of this quantile estimate is derived, which depends on the local error of the function estimate mn, and the rates of convergence of the corresponding Monte Carlo surrogate quantile estimates are analyzed for two different function estimates. The finite sample size behavior of the estimates is investigated in simulations.},
	number = {10},
	journal = {IEEE Transactions on Information Theory},
	author = {Enss, G. C. and Kohler, M. and Krzyżak, A. and Platz, R.},
	month = oct,
	year = {2016},
	keywords = {Convergence, Electronic mail, Estimation, Manganese, Monte Carlo methods, Monte Carlo surrogate quantile estimates, Nonparametric quantile estimation, Random variables, Splines (mathematics), estimate surrogate mn, estimation theory, finite sample size behavior, function estimates, general error bound, nonparametric estimation, nonparametric statistics, quantile qmn(X),α, quantisation (signal), random variable, rate of convergence, surrogate models},
	pages = {5727--5739}
}

@article{taieb_forecasting_2016,
	title = {Forecasting {Uncertainty} in {Electricity} {Smart} {Meter} {Data} by {Boosting} {Additive} {Quantile} {Regression}},
	volume = {7},
	issn = {1949-3053},
	doi = {10.1109/TSG.2016.2527820},
	abstract = {Smart electricity meters are currently deployed in millions of households to collect detailed individual electricity consumption data. Compared with traditional electricity data based on aggregated consumption, smart meter data are much more volatile and less predictable. There is a need within the energy industry for probabilistic forecasts of household electricity consumption to quantify the uncertainty of future electricity demand in order to undertake appropriate planning of generation and distribution. We propose to estimate an additive quantile regression model for a set of quantiles of the future distribution using a boosting procedure. By doing so, we can benefit from flexible and interpretable models, which include an automatic variable selection. We compare our approach with three benchmark methods on both aggregated and disaggregated scales using a smart meter data set collected from 3639 households in Ireland at 30-min intervals over a period of 1.5 years. The empirical results demonstrate that our approach based on quantile regression provides better forecast accuracy for disaggregated demand, while the traditional approach based on a normality assumption (possibly after an appropriate Box-Cox transformation) is a better approximation for aggregated demand. These results are particularly useful since more energy data will become available at the disaggregated level in the future.},
	number = {5},
	journal = {IEEE Transactions on Smart Grid},
	author = {Taieb, S. Ben and Huser, R. and Hyndman, R. J. and Genton, M. G.},
	month = sep,
	year = {2016},
	keywords = {Box-Cox transformation, Forecasting, Ireland, Load modeling, Predictive models, Probabilistic load forecasting, Probabilistic logic, Uncertainty, additive quantile regression, disaggregated demand, electricity smart meter data, energy industry, forecasting uncertainty, gradient boosting, load forecasting, probabilistic forecasts, quantile regression, regression analysis, smart electricity meters, smart meter data, smart meters, time 30 min},
	pages = {2448--2455}
}

@article{bremnes_probabilistic_2004,
	title = {Probabilistic wind power forecasts using local quantile regression},
	volume = {7},
	issn = {1099-1824},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/we.107/abstract},
	doi = {10.1002/we.107},
	abstract = {Wind power forecasts are in various ways valuable for users in decision-making processes. However, most forecasts are deterministic, and hence possibly important information about uncertainty is not available. Complete information about future production can be obtained by using probabilistic forecasts, and this article demonstrates how such forecasts can be created by means of local quantile regression. The approach has several advantages, such as no distributional assumptions and flexible inclusion of predictive information. In addition, it can be shown that, for some purposes, forecasts in terms of quantiles provide the type of information required to make optimal economic decisions. The methodology is applied to data from a wind farm in Norway. Copyright © 2004 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2017-06-19TZ},
	journal = {Wind Energy},
	author = {Bremnes, John Bjørnar},
	month = jan,
	year = {2004},
	keywords = {economic value, probabilistic forecasts, quantile regression, wind power},
	pages = {47--54}
}

@article{pinson_probabilistic_2009,
	title = {From probabilistic forecasts to statistical scenarios of short-term wind power production},
	volume = {12},
	issn = {1099-1824},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/we.284/abstract},
	doi = {10.1002/we.284},
	abstract = {Short-term (up to 2–3 days ahead) probabilistic forecasts of wind power provide forecast users with highly valuable information on the uncertainty of expected wind generation. Whatever the type of these probabilistic forecasts, they are produced on a per horizon basis, and hence do not inform on the development of the forecast uncertainty through forecast series. However, this additional information may be paramount for a large class of time-dependent and multistage decision-making problems, e.g. optimal operation of combined wind-storage systems or multiple-market trading with different gate closures. This issue is addressed here by describing a method that permits the generation of statistical scenarios of short-term wind generation that accounts for both the interdependence structure of prediction errors and the predictive distributions of wind power production. The method is based on the conversion of series of prediction errors to a multivariate Gaussian random variable, the interdependence structure of which can then be summarized by a unique covariance matrix. Such matrix is recursively estimated in order to accommodate long-term variations in the prediction error characteristics. The quality and interest of the methodology are demonstrated with an application to the test case of a multi-MW wind farm over a period of more than 2 years. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2017-06-19TZ},
	journal = {Wind Energy},
	author = {Pinson, Pierre and Madsen, Henrik and Nielsen, Henrik Aa. and Papaefthymiou, George and Klöckl, Bernd},
	month = jan,
	year = {2009},
	keywords = {Forecasting, Uncertainty, multivariate Gaussian random variable, scenarios, wind power},
	pages = {51--62}
}

@article{zhang_review_2014,
	title = {Review on probabilistic forecasting of wind power generation},
	volume = {32},
	issn = {1364-0321},
	url = {http://www.sciencedirect.com/science/article/pii/S1364032114000446},
	doi = {10.1016/j.rser.2014.01.033},
	abstract = {The randomness and intermittence of wind resources is the biggest challenge in the integration of wind power into the power system. Accurate forecasting of wind power generation is an efficient tool to deal with such problem. Conventional wind power forecasting produces a value, or the conditional expectation of wind power output at a time point in the future. However, any prediction involves inherent uncertainty. In recent years, several probabilistic forecasting approaches have been reported in wind power forecasting studies. Compared to currently wide-used point forecasts, probabilistic forecasts could provide additional quantitative information on the uncertainty associated with wind power generation. For decision-makings in the uncertainty environment, probabilistic forecasts are optimal inputs. A review of state-of-the-art methods and new developments in wind power probabilistic forecasting is presented in this paper. Firstly, three different representations of wind power uncertainty are briefly introduced. Then, different forecasting methods are discussed. These methods are classified into three categories in terms of uncertainty representation, i.e. probabilistic forecasts (parametric and non-parametric), risk index forecasts and space-time scenario forecasts. Finally, requirements and the overall framework of the uncertainty forecasting evaluation are summarized. In addition, this article also describes current challenges and future developments associated with wind power probabilistic prediction.},
	urldate = {2017-06-19TZ},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Zhang, Yao and Wang, Jianxue and Wang, Xifan},
	month = apr,
	year = {2014},
	keywords = {Forecasting evaluation, Parametric and non-parametric density, Probabilistic forecasting, Stochastic optimization, Uncertainty forecasting, Uncertainty representation, decision-making},
	pages = {255--270}
}

@article{moller_time-adaptive_2008,
	title = {Time-adaptive quantile regression},
	volume = {52},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947307002502},
	doi = {10.1016/j.csda.2007.06.027},
	abstract = {An algorithm for time-adaptive quantile regression is presented. The algorithm is based on the simplex algorithm, and the linear optimization formulation of the quantile regression problem is given. The observations have been split to allow a direct use of the simplex algorithm. The simplex method and an updating procedure are combined into a new algorithm for time-adaptive quantile regression, which generates new solutions on the basis of the old solution, leading to savings in computation time. The suggested algorithm is tested against a static quantile regression model on a data set with wind power production, where the models combine splines and quantile regression. The comparison indicates superior performance for the time-adaptive quantile regression in all the performance parameters considered.11The Matlab algorithms can be obtained from http://www.imm.dtu.dk/∼jkm.},
	number = {3},
	urldate = {2017-06-19TZ},
	journal = {Computational Statistics \& Data Analysis},
	author = {Møller, Jan Kloppenborg and Nielsen, Henrik Aalborg and Madsen, Henrik},
	month = jan,
	year = {2008},
	keywords = {Simplex, Time-adaptive, quantile regression, wind power},
	pages = {1292--1303}
}

@article{machado_counterfactual_2005,
	title = {Counterfactual decomposition of changes in wage distributions using quantile regression},
	volume = {20},
	issn = {1099-1255},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/jae.788/abstract},
	doi = {10.1002/jae.788},
	abstract = {We propose a method to decompose the changes in the wage distribution over a period of time in several factors contributing to those changes. The method is based on the estimation of marginal wage distributions consistent with a conditional distribution estimated by quantile regression as well as with any hypothesized distribution for the covariates. Comparing the marginal distributions implied by different distributions for the covariates, one is then able to perform counterfactual exercises. The proposed methodology enables the identification of the sources of the increased wage inequality observed in most countries. Specifically, it decomposes the changes in the wage distribution over a period of time into several factors contributing to those changes, namely by discriminating between changes in the characteristics of the working population and changes in the returns to these characteristics. We apply this methodology to Portuguese data for the period 1986–1995, and find that the observed increase in educational levels contributed decisively towards greater wage inequality. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {4},
	urldate = {2017-06-19TZ},
	journal = {Journal of Applied Econometrics},
	author = {Machado, José A. F. and Mata, José},
	month = may,
	year = {2005},
	pages = {445--465}
}

@article{lin_variable_2013,
	title = {Variable {Selection} for {Nonparametric} {Quantile} {Regression} via {Smoothing} {Spline} {AN} {OVA}},
	volume = {2},
	issn = {0038-9986},
	doi = {10.1002/sta4.33},
	abstract = {Quantile regression provides a more thorough view of the effect of covariates on a response. Nonparametric quantile regression has become a viable alternative to avoid restrictive parametric assumption. The problem of variable selection for quantile regression is challenging, since important variables can influence various quantiles in different ways. We tackle the problem via regularization in the context of smoothing spline ANOVA models. The proposed sparse nonparametric quantile regression (SNQR) can identify important variables and provide flexible estimates for quantiles. Our numerical study suggests the promising performance of the new procedure in variable selection and function estimation. Supplementary materials for this article are available online.},
	language = {eng},
	number = {1},
	journal = {Stat},
	author = {Lin, Chen-Yen and Bondell, Howard and Zhang, Hao Helen and Zou, Hui},
	year = {2013},
	pmid = {24554792},
	pmcid = {PMC3926212},
	keywords = {COSSO, Kernel Quantile Regression, Model Selection, Reproducing Kernel Hilbert Space},
	pages = {255--268}
}

@article{li_quantile_2007,
	title = {Quantile {Regression} in {Reproducing} {Kernel} {Hilbert} {Spaces}},
	volume = {102},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/27639837},
	abstract = {In this article we consider quantile regression in reproducing kernel Hilbert spaces, which we call kernel quantile regression (KQR). We make three contributions: (1) we propose an efficient algorithm that computes the entire solution path of the KQR, with essentially the same computational cost as fitting one KQR model; (2) we derive a simple formula for the effective dimension of the KQR model, which allows convenient selection of the regularization parameter; and (3) we develop an asymptotic theory for the KQR model.},
	number = {477},
	urldate = {2017-06-19TZ},
	journal = {Journal of the American Statistical Association},
	author = {Li, Youjuan and Liu, Yufeng and Zhu, Ji},
	year = {2007},
	pages = {255--268}
}

@article{li_l1-norm_2008,
	title = {L1-{Norm} {Quantile} {Regression}},
	volume = {17},
	issn = {1061-8600},
	url = {http://dx.doi.org/10.1198/106186008X289155},
	doi = {10.1198/106186008X289155},
	abstract = {Classical regression methods have focused mainly on estimating conditional mean functions. In recent years, however, quantile regression has emerged as a comprehensive approach to the statistical analysis of response models. In this article we consider the L1-norm (LASSO) regularized quantile regression (L1-norm QR), which uses the sum of the absolute values of the coefficients as the penalty. The L1-norm penalty has the advantage of simultaneously controlling the variance of the fitted coefficients and performing automatic variable selection. We propose an efficient algorithm that computes the entire solution path of the L1-norm QR. Furthermore, we derive an estimate for the effective dimension of the L1-norm QR model, which allows convenient selection of the regularization parameter.},
	number = {1},
	urldate = {2017-06-19TZ},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Li, Youjuan and Zhu, Ji},
	month = mar,
	year = {2008},
	pages = {163--185}
}

@article{haque_hybrid_2014,
	title = {A {Hybrid} {Intelligent} {Model} for {Deterministic} and {Quantile} {Regression} {Approach} for {Probabilistic} {Wind} {Power} {Forecasting}},
	volume = {29},
	issn = {0885-8950},
	doi = {10.1109/TPWRS.2014.2299801},
	abstract = {With rapid increase in wind power penetration into the power grid, wind power forecasting is becoming increasingly important to power system operators and electricity market participants. The majority of the wind forecasting tools available in the literature provide deterministic prediction, but given the variability and uncertainty of wind, such predictions limit the use of the existing tools for decision-making under uncertain conditions. As a result, probabilistic forecasting, which provides information on uncertainty associated with wind power forecasting, is gaining increased attention. This paper presents a novel hybrid intelligent algorithm for deterministic wind power forecasting that utilizes a combination of wavelet transform (WT) and fuzzy ARTMAP (FA) network, which is optimized by using firefly (FF) optimization algorithm. In addition, support vector machine (SVM) classifier is used to minimize the wind power forecast error obtained from WT+FA+FF. The paper also presents a probabilistic wind power forecasting algorithm using quantile regression method. It uses the wind power forecast results obtained from the proposed hybrid deterministic WT+FA+FF+SVM model to evaluate the probabilistic forecasting performance. The performance of the proposed forecasting model is assessed utilizing wind power data from the Cedar Creek wind farm in Colorado.},
	number = {4},
	journal = {IEEE Transactions on Power Systems},
	author = {Haque, A. U. and Nehrir, M. H. and Mandal, P.},
	month = jul,
	year = {2014},
	keywords = {ART neural nets, Artificial neural networks, Cedar Creek wind farm, Deterministic and probabilistic wind power forecasting, FF optimization algorithm, Forecasting, Predictive models, Probabilistic logic, Wind forecasting, Wind power generation, decision-making, deterministic regression approach, electricity market participants, firefly, firefly optimization algorithm, fuzzy ARTMAP, fuzzy ARTMAP network, fuzzy neural nets, hybrid deterministic WT-FA-FF-SVM model, hybrid intelligent model, optimisation, pattern classification, power engineering computing, power generation economics, power grid, power grids, power markets, power system operators, probabilistic wind power forecasting algorithm, probability, quantile regression approach, regression analysis, support vector machine classifier, support vector machines, uncertain conditions, wavelet transform, wavelet transforms, wind power forecast error, wind power penetration, wind power plants},
	pages = {1663--1672}
}

@misc{anastasiades_quantile_nodate,
	title = {Quantile {Forecasting} of {Wind} {Power} {Using} {Variability} {Indices}},
	url = {http://www.mdpi.com/1996-1073/6/2/662},
	urldate = {2017-06-08TZ},
	author = {Anastasiades}
}

@misc{bosch_convergent_nodate,
	title = {A convergent algorithm for quantile regression with smoothing splines - {ScienceDirect}},
	url = {http://www.sciencedirect.com/science/article/pii/016794739400018E},
	urldate = {2017-06-05TZ},
	author = {Bosch}
}

@article{chernozhukov_quantile_2010,
	title = {Quantile and {Probability} {Curves} {Without} {Crossing}},
	volume = {78},
	issn = {1468-0262},
	url = {http://onlinelibrary.wiley.com/doi/10.3982/ECTA7880/abstract},
	doi = {10.3982/ECTA7880},
	abstract = {This paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem (Bassett and Koenker (1982)). The method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. We show that the rearranged curve is closer to the true quantile curve than the original curve in finite samples, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. We also establish validity of the bootstrap for estimating the limit law of the entire rearranged curve and its functionals. Our limit results are generic in that they apply to every estimator of a monotone function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. Consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. We illustrate the results with an application to estimation of structural distribution and quantile functions using data on Vietnam veteran status and earnings.},
	language = {en},
	number = {3},
	urldate = {2017-06-19TZ},
	journal = {Econometrica},
	author = {Chernozhukov, Victor and Fernández-Val, Iván and Galichon, Alfred},
	month = may,
	year = {2010},
	keywords = {Conditional quantiles, functional delta method, isotonic regression, monotonicity problem, rearrangement, structural quantiles},
	pages = {1093--1125}
}

@article{gallego-castillo_-line_2016,
	title = {On-line quantile regression in the {RKHS} ({Reproducing} {Kernel} {Hilbert} {Space}) for operational probabilistic forecasting of wind power},
	volume = {113},
	issn = {0360-5442},
	url = {http://www.sciencedirect.com/science/article/pii/S0360544216309793},
	doi = {10.1016/j.energy.2016.07.055},
	abstract = {Wind power probabilistic forecast is being used as input in several decision-making problems, such as stochastic unit commitment, operating reserve setting and electricity market bidding. This work introduces a new on-line quantile regression model based on the Reproducing Kernel Hilbert Space (RKHS) framework. Its application to the field of wind power forecasting involves a discussion on the choice of the bias term of the quantile models, and the consideration of the operational framework in order to mimic real conditions. Benchmark against linear and splines quantile regression models was performed for a real case study during a 18 months period. Model parameter selection was based on k-fold crossvalidation. Results showed a noticeable improvement in terms of calibration, a key criterion for the wind power industry. Modest improvements in terms of Continuous Ranked Probability Score (CRPS) were also observed for prediction horizons between 6 and 20 h ahead.},
	urldate = {2017-06-19TZ},
	journal = {Energy},
	author = {Gallego-Castillo, Cristobal and Bessa, Ricardo and Cavalcante, Laura and Lopez-Garcia, Oscar},
	month = oct,
	year = {2016},
	keywords = {On-line, Probabilistic forecast, Reproducing Kernel Hilbert Space (RKHS), Short-term, quantile regression, wind power},
	pages = {355--365}
}

@article{bacher_online_2009,
	title = {Online short-term solar power forecasting},
	volume = {83},
	issn = {0038-092X},
	url = {http://www.sciencedirect.com/science/article/pii/S0038092X09001364},
	doi = {10.1016/j.solener.2009.05.016},
	abstract = {This paper describes a new approach to online forecasting of power production from PV systems. The method is suited to online forecasting in many applications and in this paper it is used to predict hourly values of solar power for horizons of up to 36h. The data used is 15-min observations of solar power from 21 PV systems located on rooftops in a small village in Denmark. The suggested method is a two-stage method where first a statistical normalization of the solar power is obtained using a clear sky model. The clear sky model is found using statistical smoothing techniques. Then forecasts of the normalized solar power are calculated using adaptive linear time series models. Both autoregressive (AR) and AR with exogenous input (ARX) models are evaluated, where the latter takes numerical weather predictions (NWPs) as input. The results indicate that for forecasts up to 2h ahead the most important input is the available observations of solar power, while for longer horizons NWPs are the most important input. A root mean square error improvement of around 35\% is achieved by the ARX model compared to a proposed reference model.},
	number = {10},
	urldate = {2017-06-19TZ},
	journal = {Solar Energy},
	author = {Bacher, Peder and Madsen, Henrik and Nielsen, Henrik Aalborg},
	month = oct,
	year = {2009},
	keywords = {Clear sky model, Forecasting, Numerical weather predictions, Photovoltaic, Prediction, Recursive least squares, Solar power, Time series, quantile regression},
	pages = {1772--1783}
}

@article{aue_piecewise_2017,
	title = {Piecewise quantile autoregressive modeling for nonstationary time series},
	volume = {23},
	issn = {1350-7265},
	url = {http://arxiv.org/abs/1609.08882},
	doi = {10.3150/14-BEJ671},
	abstract = {We develop a new methodology for the fitting of nonstationary time series that exhibit nonlinearity, asymmetry, local persistence and changes in location scale and shape of the underlying distribution. In order to achieve this goal, we perform model selection in the class of piecewise stationary quantile autoregressive processes. The best model is defined in terms of minimizing a minimum description length criterion derived from an asymmetric Laplace likelihood. Its practical minimization is done with the use of genetic algorithms. If the data generating process follows indeed a piecewise quantile autoregression structure, we show that our method is consistent for estimating the break points and the autoregressive parameters. Empirical work suggests that the proposed method performs well in finite samples.},
	number = {1},
	urldate = {2017-06-08TZ},
	journal = {Bernoulli},
	author = {Aue, Alexander and Cheung, Rex C. Y. and Lee, Thomas C. M. and Zhong, Ming},
	month = feb,
	year = {2017},
	note = {arXiv: 1609.08882},
	keywords = {Mathematics - Statistics Theory},
	pages = {1--22}
}
@techreport{chao_quantile_2012,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Quantile {Regression} in {Risk} {Calibration}},
	url = {https://papers.ssrn.com/abstract=2894219},
	abstract = {Financial risk control has always been challenging and becomes now an even harder problem as joint extreme events occur more frequently. For decision makers and},
	number = {ID 2894219},
	urldate = {2017-06-08TZ},
	institution = {Social Science Research Network},
	author = {Chao, Shih-Kang and Härdle, Wolfgang K. and Wang, Weining},
	month = jan,
	year = {2012},
	keywords = {CoVaR, Value-at-Risk, locally linear quantile regression, partial linear model, quantile regression, semiparametric model}
}

@article{bremnes_probabilistic_2004,
	title = {Probabilistic wind power forecasts using local quantile regression},
	volume = {7},
	issn = {1099-1824},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/we.107/abstract},
	doi = {10.1002/we.107},
	abstract = {Wind power forecasts are in various ways valuable for users in decision-making processes. However, most forecasts are deterministic, and hence possibly important information about uncertainty is not available. Complete information about future production can be obtained by using probabilistic forecasts, and this article demonstrates how such forecasts can be created by means of local quantile regression. The approach has several advantages, such as no distributional assumptions and flexible inclusion of predictive information. In addition, it can be shown that, for some purposes, forecasts in terms of quantiles provide the type of information required to make optimal economic decisions. The methodology is applied to data from a wind farm in Norway. Copyright © 2004 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2017-06-08TZ},
	journal = {Wind Energy},
	author = {Bremnes, John Bjørnar},
	month = jan,
	year = {2004},
	keywords = {economic value, probabilistic forecasts, quantile regression, wind power},
	pages = {47--54}
}

@article{taieb_forecasting_2016,
	title = {Forecasting {Uncertainty} in {Electricity} {Smart} {Meter} {Data} by {Boosting} {Additive} {Quantile} {Regression}},
	volume = {7},
	issn = {1949-3053},
	doi = {10.1109/TSG.2016.2527820},
	abstract = {Smart electricity meters are currently deployed in millions of households to collect detailed individual electricity consumption data. Compared with traditional electricity data based on aggregated consumption, smart meter data are much more volatile and less predictable. There is a need within the energy industry for probabilistic forecasts of household electricity consumption to quantify the uncertainty of future electricity demand in order to undertake appropriate planning of generation and distribution. We propose to estimate an additive quantile regression model for a set of quantiles of the future distribution using a boosting procedure. By doing so, we can benefit from flexible and interpretable models, which include an automatic variable selection. We compare our approach with three benchmark methods on both aggregated and disaggregated scales using a smart meter data set collected from 3639 households in Ireland at 30-min intervals over a period of 1.5 years. The empirical results demonstrate that our approach based on quantile regression provides better forecast accuracy for disaggregated demand, while the traditional approach based on a normality assumption (possibly after an appropriate Box-Cox transformation) is a better approximation for aggregated demand. These results are particularly useful since more energy data will become available at the disaggregated level in the future.},
	number = {5},
	journal = {IEEE Transactions on Smart Grid},
	author = {Taieb, S. Ben and Huser, R. and Hyndman, R. J. and Genton, M. G.},
	month = sep,
	year = {2016},
	keywords = {Box-Cox transformation, Forecasting, Ireland, Load modeling, Predictive models, Probabilistic load forecasting, Probabilistic logic, Uncertainty, additive quantile regression, disaggregated demand, electricity smart meter data, energy industry, forecasting uncertainty, gradient boosting, load forecasting, probabilistic forecasts, quantile regression, regression analysis, smart electricity meters, smart meter data, smart meters, time 30 min},
	pages = {2448--2455}
}

@article{gaglianone_constructing_2014,
	title = {Constructing {Optimal} {Density} {Forecasts} from {Point} {Forecast} {Combinations}},
	volume = {29},
	issn = {1099-1255},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/jae.2352/abstract},
	doi = {10.1002/jae.2352},
	abstract = {Decision makers often observe point forecasts of the same variable computed, for instance, by commercial banks, IMF and the World Bank, but the econometric models used by such institutions are frequently unknown. This paper shows how to use the information available on point forecasts to compute optimal density forecasts. Our idea builds upon the combination of point forecasts under general loss functions and unknown forecast error distributions. We use real-time data to forecast the density of US inflation. The results indicate that the proposed method materially improves the real-time accuracy of density forecasts vis-à-vis those from the (unknown) individual econometric models. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2017-06-07TZ},
	journal = {Journal of Applied Econometrics},
	author = {Gaglianone, Wagner Piazza and Lima, Luiz Renato},
	month = aug,
	year = {2014},
	pages = {736--757}
}

@misc{noauthor_unconditional_nodate,
	title = {Unconditional {Quantile} {Regressions} - {Firpo} - 2009 - {Econometrica} - {Wiley} {Online} {Library}},
	url = {http://onlinelibrary.wiley.com/doi/10.3982/ECTA6822/full},
	urldate = {2017-06-07TZ}
}

@article{belloni_l1-penalized_2009,
	title = {L1-{Penalized} {Quantile} {Regression} in {High}-{Dimensional} {Sparse} {Models}},
	url = {http://arxiv.org/abs/0904.2931},
	abstract = {We consider median regression and, more generally, a possibly infinite collection of quantile regressions in high-dimensional sparse models. In these models the overall number of regressors \$p\$ is very large, possibly larger than the sample size \$n\$, but only \$s\$ of these regressors have non-zero impact on the conditional quantile of the response variable, where \$s\$ grows slower than \$n\$. We consider quantile regression penalized by the \${\textbackslash}ell\_1\$-norm of coefficients (\${\textbackslash}ell\_1\$-QR). First, we show that \${\textbackslash}ell\_1\$-QR is consistent at the rate \${\textbackslash}sqrt\{s/n\} {\textbackslash}sqrt\{{\textbackslash}log p\}\$. The overall number of regressors \$p\$ affects the rate only through the \${\textbackslash}log p\$ factor, thus allowing nearly exponential growth in the number of zero-impact regressors. The rate result holds under relatively weak conditions, requiring that \$s/n\$ converges to zero at a super-logarithmic speed and that regularization parameter satisfies certain theoretical constraints. Second, we propose a pivotal, data-driven choice of the regularization parameter and show that it satisfies these theoretical constraints. Third, we show that \${\textbackslash}ell\_1\$-QR correctly selects the true minimal model as a valid submodel, when the non-zero coefficients of the true model are well separated from zero. We also show that the number of non-zero coefficients in \${\textbackslash}ell\_1\$-QR is of same stochastic order as \$s\$. Fourth, we analyze the rate of convergence of a two-step estimator that applies ordinary quantile regression to the selected model. Fifth, we evaluate the performance of \${\textbackslash}ell\_1\$-QR in a Monte-Carlo experiment, and illustrate its use on an international economic growth application.},
	urldate = {2017-06-06TZ},
	journal = {arXiv:0904.2931 [math, stat]},
	author = {Belloni, Alexandre and Chernozhukov, Victor},
	month = apr,
	year = {2009},
	note = {arXiv: 0904.2931},
	keywords = {Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Methodology}
}

@article{gaglianone_evaluating_2011,
	title = {Evaluating {Value}-at-{Risk} {Models} via {Quantile} {Regression}},
	volume = {29},
	issn = {0735-0015},
	url = {http://dx.doi.org/10.1198/jbes.2010.07318},
	doi = {10.1198/jbes.2010.07318},
	abstract = {This article is concerned with evaluating Value-at-Risk estimates. It is well known that using only binary variables, such as whether or not there was an exception, sacrifices too much information. However, most of the specification tests (also called backtests) available in the literature, such as Christoffersen (1998) and Engle and Manganelli (2004) are based on such variables. In this article we propose a new backtest that does not rely solely on binary variables. It is shown that the new backtest provides a sufficient condition to assess the finite sample performance of a quantile model whereas the existing ones do not. The proposed methodology allows us to identify periods of an increased risk exposure based on a quantile regression model (Koenker and Xiao 2002). Our theoretical findings are corroborated through a Monte Carlo simulation and an empirical exercise with daily S\&P500 time series.},
	number = {1},
	journal = {Journal of Business \& Economic Statistics},
	author = {Gaglianone, Wagner Piazza and Lima, Luiz Renato and Linton, Oliver and Smith, Daniel R.},
	month = jan,
	year = {2011},
	keywords = {Backtesting, Risk exposure},
	pages = {150--160}
}

@article{carlier_vector_2014,
	title = {Vector {Quantile} {Regression}: {An} {Optimal} {Transport} {Approach}},
	shorttitle = {Vector {Quantile} {Regression}},
	url = {http://arxiv.org/abs/1406.4643},
	abstract = {We propose a notion of conditional vector quantile function and a vector quantile regression. A {\textbackslash}emph\{conditional vector quantile function\} (CVQF) of a random vector \$Y\$, taking values in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ given covariates \$Z=z\$, taking values in \${\textbackslash}mathbb\{R\}\% {\textasciicircum}k\$, is a map \$u {\textbackslash}longmapsto Q\_\{Y{\textbackslash}mid Z\}(u,z)\$, which is monotone, in the sense of being a gradient of a convex function, and such that given that vector \$U\$ follows a reference non-atomic distribution \$F\_U\$, for instance uniform distribution on a unit cube in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$, the random vector \$Q\_\{Y{\textbackslash}mid Z\}(U,z)\$ has the distribution of \$Y\$ conditional on \$Z=z\$. Moreover, we have a strong representation, \$Y = Q\_\{Y{\textbackslash}mid Z\}(U,Z)\$ almost surely, for some version of \$U\$. The {\textbackslash}emph\{vector quantile regression\} (VQR) is a linear model for CVQF of \$Y\$ given \$Z\$. Under correct specification, the notion produces strong representation, \$Y={\textbackslash}beta {\textbackslash}left(U{\textbackslash}right) {\textasciicircum}{\textbackslash}top f(Z)\$, for \$f(Z)\$ denoting a known set of transformations of \$Z\$, where \$u {\textbackslash}longmapsto {\textbackslash}beta(u){\textasciicircum}{\textbackslash}top f(Z)\$ is a monotone map, the gradient of a convex function, and the quantile regression coefficients \$u {\textbackslash}longmapsto {\textbackslash}beta(u)\$ have the interpretations analogous to that of the standard scalar quantile regression. As \$f(Z)\$ becomes a richer class of transformations of \$Z\$, the model becomes nonparametric, as in series modelling. A key property of VQR is the embedding of the classical Monge-Kantorovich's optimal transportation problem at its core as a special case. In the classical case, where \$Y\$ is scalar, VQR reduces to a version of the classical QR, and CVQF reduces to the scalar conditional quantile function. An application to multiple Engel curve estimation is considered.},
	journal = {arXiv:1406.4643 [stat]},
	author = {Carlier, Guillaume and Chernozhukov, Victor and Galichon, Alfred},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.4643},
	keywords = {49Q20, 49Q10, 90B20, Statistics - Methodology}
}

@article{cai_regression_2002,
	title = {Regression {Quantiles} for {Time} {Series}},
	volume = {18},
	issn = {0266-4666},
	url = {http://www.jstor.org/stable/3533031},
	abstract = {In this paper we study nonparametric estimation of regression quantiles for time series data by inverting a weighted Nadaraya-Watson (WNW) estimator of conditional distribution function, which was first used by Hall, Wolff, and Yao (1999 "Journal of the American Statistical Association" 94, 154-163). First, under some regularity conditions, we establish the asymptotic normality and weak consistency of the WNW conditional distribution estimator for α-mixing time series at both boundary and interior points, and we show that the WNW conditional distribution estimator not only preserves the bias, variance, and, more important, automatic good boundary behavior properties of local linear "double-kernel" estimators introduced by Yu and Jones (1998, "Journal of the American Statistical Association" 93, 228-237), but also has the additional advantage of always being a distribution itself. Second, it is shown that under some regularity conditions, the WNW conditional quantile estimator is weakly consistent and normally distributed and that it inherits all good properties from the WNW conditional distribution estimator. A small simulation study is carried out to illustrate the performance of the estimates, and a real example is also used to demonstrate the methodology.},
	number = {1},
	journal = {Econometric Theory},
	author = {Cai, Zongwu},
	year = {2002},
	pages = {169--192}
}

@article{bertsimas_least_2014,
	title = {Least quantile regression via modern optimization},
	volume = {42},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1310.8625},
	doi = {10.1214/14-AOS1223},
	abstract = {We address the Least Quantile of Squares (LQS) (and in particular the Least Median of Squares) regression problem using modern optimization methods. We propose a Mixed Integer Optimization (MIO) formulation of the LQS problem which allows us to find a provably global optimal solution for the LQS problem. Our MIO framework has the appealing characteristic that if we terminate the algorithm early, we obtain a solution with a guarantee on its sub-optimality. We also propose continuous optimization methods based on first-order subdifferential methods, sequential linear optimization and hybrid combinations of them to obtain near optimal solutions to the LQS problem. The MIO algorithm is found to benefit significantly from high quality solutions delivered by our continuous optimization based methods. We further show that the MIO approach leads to (a) an optimal solution for any dataset, where the data-points \$(y\_i,{\textbackslash}mathbf\{x\}\_i)\$'s are not necessarily in general position, (b) a simple proof of the breakdown point of the LQS objective value that holds for any dataset and (c) an extension to situations where there are polyhedral constraints on the regression coefficient vector. We report computational results with both synthetic and real-world datasets showing that the MIO algorithm with warm starts from the continuous optimization methods solve small (\$n=100\$) and medium (\$n=500\$) size problems to provable optimality in under two hours, and outperform all publicly available methods for large-scale (\$n=\{\}\$10,000) LQS problems.},
	number = {6},
	journal = {The Annals of Statistics},
	author = {Bertsimas, Dimitris and Mazumder, Rahul},
	month = dec,
	year = {2014},
	note = {arXiv: 1310.8625},
	keywords = {Statistics - Computation},
	pages = {2494--2525}
}

@article{bertsimas_best_2015,
	title = {Best {Subset} {Selection} via a {Modern} {Optimization} {Lens}},
	url = {http://arxiv.org/abs/1507.03133},
	abstract = {In the last twenty-five years (1990-2014), algorithmic advances in integer optimization combined with hardware improvements have resulted in an astonishing 200 billion factor speedup in solving Mixed Integer Optimization (MIO) problems. We present a MIO approach for solving the classical best subset selection problem of choosing \$k\$ out of \$p\$ features in linear regression given \$n\$ observations. We develop a discrete extension of modern first order continuous optimization methods to find high quality feasible solutions that we use as warm starts to a MIO solver that finds provably optimal solutions. The resulting algorithm (a) provides a solution with a guarantee on its suboptimality even if we terminate the algorithm early, (b) can accommodate side constraints on the coefficients of the linear regression and (c) extends to finding best subset solutions for the least absolute deviation loss function. Using a wide variety of synthetic and real datasets, we demonstrate that our approach solves problems with \$n\$ in the 1000s and \$p\$ in the 100s in minutes to provable optimality, and finds near optimal solutions for \$n\$ in the 100s and \$p\$ in the 1000s in minutes. We also establish via numerical experiments that the MIO approach performs better than \{{\textbackslash}texttt \{Lasso\}\} and other popularly used sparse learning procedures, in terms of achieving sparse solutions with good predictive power.},
	journal = {arXiv:1507.03133 [math, stat]},
	author = {Bertsimas, Dimitris and King, Angela and Mazumder, Rahul},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.03133},
	keywords = {Mathematics - Optimization and Control, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology}
}

@misc{center_for_history_and_new_media_zotero_nodate,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}}
}

@article{horvath_testing_2014,
	title = {Testing stationarity of functional time series},
	volume = {179},
	url = {http://ideas.repec.org/a/eee/econom/v179y2014i1p66-82.html},
	abstract = {Economic and financial data often take the form of a collection of curves observed consecutively over time. Examples include, intraday price curves, yield and term structure curves, and intraday volatility curves. Such curves can be viewed as a time series of functions. A fundamental issue that must be addressed, before an attempt is made to statistically model such data, is whether these curves, perhaps suitably transformed, form a stationary functional time series. This paper formalizes the assumption of stationarity in the context of functional time series and proposes several procedures to test the null hypothesis of stationarity. The tests are nontrivial extensions of the broadly used tests in the KPSS family. The properties of the tests under several alternatives, including change-point and I(1), are studied, and new insights, present only in the functional setting are uncovered. The theory is illustrated by a small simulation study and an application to intraday price curves.},
	number = {1},
	urldate = {2014-05-04TZ},
	journal = {Journal of Econometrics},
	author = {Horváth, Lajos and Kokoszka, Piotr and Rice, Gregory},
	year = {2014},
	keywords = {Change point, Functional data, Integrated time series, Intraday price curves, Test of stationarity},
	pages = {66--82}
}

@article{diebold_comparing_2002,
	title = {Comparing predictive accuracy},
	volume = {20},
	url = {http://amstat.tandfonline.com/doi/full/10.1198/073500102753410444},
	number = {1},
	urldate = {2014-05-04TZ},
	journal = {Journal of Business \& economic statistics},
	author = {Diebold, Francis X. and Mariano, Robert S.},
	year = {2002}
}

@book{noauthor_principal_nodate,
	title = {Principal {Component} {Analysis}},
	url = {http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-95442-4},
	abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques, it continues to be the subject of much research, ranging from new model-based approaches to ...},
	urldate = {2014-05-04TZ},
	keywords = {Principal Component Analysis, Statistical Theory and Methods}
}

@article{ferraty_nonparametric_2007,
	title = {Nonparametric {Regression} on {Functional} {Data}: {Inference} and {Practical} {Aspects}},
	volume = {49},
	issn = {1467-842X},
	shorttitle = {Nonparametric {Regression} on {Functional} {Data}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-842X.2007.00480.x/abstract},
	doi = {10.1111/j.1467-842X.2007.00480.x},
	abstract = {We consider the problem of predicting a real random variable from a functional explanatory variable. The problem is tackled using a nonparametric kernel approach, which has been recently adapted to this functional context. We derive theoretical results from a deep asymptotic analysis of the behaviour of the estimate, including mean squared convergence (with rates and precise evaluation of the constant terms) as well as asymptotic distribution. Practical use of these results relies on the ability to estimate these constants. Some perspectives in this direction are discussed. In particular, a functional version of wild bootstrapping ideas is proposed and used both on simulated and real functional datasets.},
	language = {en},
	number = {3},
	urldate = {2014-05-04TZ},
	journal = {Australian \& New Zealand Journal of Statistics},
	author = {Ferraty, Frédéric and Mas, André and Vieu, Philippe},
	month = sep,
	year = {2007},
	keywords = {Functional data, asymptotic normality, nonparametric model, quadratic error, regression, wild functional bootstrap},
	pages = {267--286}
}

@book{ramsay_functional_2009,
	title = {Functional data analysis with {R} and {MATLAB}},
	isbn = {9780387981857},
	abstract = {Scientists often collect samples of curves and other functional observations, and develop models where parameters are also functions. This title provides computer code in both the R and Matlab languages for a set of data analyses that showcase functional data analysis techniques.},
	language = {en},
	publisher = {Springer},
	author = {Ramsay, J. James O. and Hooker, Giles and Graves, Spencer},
	year = {2009},
	keywords = {Computers / Mathematical \& Statistical Software, Computers / Mathematical \& Statistical Software, Computers / Programming Languages / General, Computers / Programming Languages / General, Mathematics / Probability \& Statistics / Multivariate Analysis, Mathematics / Probability \& Statistics / Multivariate Analysis}
}

@book{james_o._ramsay_applied_nodate,
	title = {Applied {Functional} {Data} {Analysis}: {Methods} and {Case} {Studies}},
	publisher = {Springer},
	author = {{James O. Ramsay} and Silverman, B. W.}
}

@book{ramsay_functional_2005,
	title = {Functional {Data} {Analysis}},
	isbn = {9780387400808},
	abstract = {Scientists today collect samples of curves and other functional observations. This monograph presents many ideas and techniques for such data. Included are expressions in the functional domain of such classics as linear regression, principal components analysis, linear modelling, and canonical correlation analysis, as well as specifically functional techniques such as curve registration and principal differential analysis. Data arising in real applications are used throughout for both motivation and illustration, showing how functional approaches allow us to see new things, especially by exploiting the smoothness of the processes generating the data. The data sets exemplify the wide scope of functional data analysis; they are drwan from growth analysis, meterology, biomechanics, equine science, economics, and medicine.The book presents novel statistical technology while keeping the mathematical level widely accessible. It is designed to appeal to students, to applied data analysts, and to experienced researchers; it will have value both within statistics and across a broad spectrum of other fields. Much of the material is based on the authors' own work, some of which appears here for the first time.Jim Ramsay is Professor of Psychology at McGill University and is an international authority on many aspects of multivariate analysis. He draws on his collaboration with researchers in speech articulation, motor control, meteorology, psychology, and human physiology to illustrate his technical contributions to functional data analysis in a wide range of statistical and application journals.Bernard Silverman, author of the highly regarded "Density Estimation for Statistics and Data Analysis," and coauthor of "Nonparametric Regression and Generalized Linear Models: A Roughness Penalty Approach," is Professor of Statistics at Bristol University. His published work on smoothing methods and other aspects of applied, computational, and theoretical statistics has been recognized by the Presidents' Award of the Committee of Presidents of Statistical Societies, and the award of two Guy Medals by the Royal Statistical Society.},
	language = {en},
	publisher = {Springer},
	author = {Ramsay, James O. and Silverman, B. W.},
	month = jun,
	year = {2005},
	keywords = {Business \& Economics / Statistics, Business \& Economics / Statistics, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Multivariate Analysis, Mathematics / Probability \& Statistics / Multivariate Analysis}
}

@book{vieu_nonparametric_2006,
	title = {Nonparametric functional data analysis},
	isbn = {9780387366203},
	abstract = {Modern apparatuses allow us to collect samples of functional data, mainly curves but also images. On the other hand, nonparametric statistics produces useful tools for standard data exploration. This book links these two fields of modern statistics by explaining how functional data can be studied through parameter-free statistical ideas. This book starts from theoretical foundations including functional nonparametric modeling, description of the mathematical framework, construction of the statistical methods, and statements of their asymptotic behaviors. It proceeds to computational issues including R and S-PLUS routines. Several functional datasets in chemometrics, econometrics, and pattern recognition are used to emphasize the wide scope of nonparametric functional data analysis in applied sciences. The companion Web site includes R and S-PLUS routines, command lines for reproducing examples presented in the book, and the functional datasets.Rather than set application against theory, this book is really an interface of these two features of statistics. A special effort has been made in writing this book to accommodate several levels of reading. The computational aspects are oriented toward practitioners whereas open problems emerging from this new field of statistics will attract Ph.D. students and academic researchers. Finally, this book is also accessible to graduate students starting in the area of functional statistics.},
	language = {en},
	publisher = {Springer},
	author = {Vieu, Philippe and Ferraty, Frederic},
	year = {2006},
	keywords = {Business \& Economics / Econometrics, Business \& Economics / Econometrics, Computers / Computer Science, Computers / Computer Science, Mathematics / Functional Analysis, Mathematics / Functional Analysis, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / General, Science / Earth Sciences / Geology, Science / Earth Sciences / Geology, Science / Environmental Science, Science / Environmental Science}
}

@book{wand_kernel_1995,
	title = {Kernel smoothing},
	isbn = {9780412552700},
	abstract = {Kernel smoothing refers to a general methodology for recovery of underlying structure in data sets. The basic principle is that local averaging or smoothing is performed with respect to a kernel function.This book provides uninitiated readers with a feeling for the principles, applications, and analysis of kernel smoothers. This is facilitated by the authors' focus on the simplest settings, namely density estimation and nonparametric regression. They pay particular attention to the problem of choosing the smoothing parameter of a kernel smoother, and also treat the multivariate case in detail. Kernal Smoothing is self-contained and assumes only a basic knowledge of statistics, calculus, and matrix algebra. It is an invaluable introduction to the main ideas of kernel estimation for students and researchers from other discipline and provides a comprehensive reference for those familiar with the topic.},
	language = {en},
	publisher = {Chapman and Hall},
	author = {Wand, M. Matt P. and Jones, M. M. Chris},
	year = {1995},
	keywords = {\&, /, General, Mathematics, Mathematics / Probability \& Statistics / General, Probability, Statistics}
}

@article{shang_survey_2014,
	title = {A survey of functional principal component analysis},
	volume = {98},
	issn = {1863-8171, 1863-818X},
	url = {http://link.springer.com/article/10.1007/s10182-013-0213-1},
	doi = {10.1007/s10182-013-0213-1},
	abstract = {Advances in data collection and storage have tremendously increased the presence of functional data, whose graphical representations are curves, images or shapes. As a new area of statistics, functional data analysis extends existing methodologies and theories from the realms of functional analysis, generalized linear model, multivariate data analysis, nonparametric statistics, regression models and many others. From both methodological and practical viewpoints, this paper provides a review of functional principal component analysis, and its use in explanatory analysis, modeling and forecasting, and classification of functional data.},
	language = {en},
	number = {2},
	urldate = {2014-05-04TZ},
	journal = {AStA Advances in Statistical Analysis},
	author = {Shang, Han Lin},
	month = apr,
	year = {2014},
	keywords = {Dimension reduction, Econometrics, Explanatory analysis, Functional data clustering, Functional data forecasting, Functional data modeling, Probability Theory and Stochastic Processes, Statistics for Business/Economics/Mathematical Finance/Insurance, Statistics, general},
	pages = {121--142}
}

@article{locantore_robust_1999,
	title = {Robust principal component analysis for functional data},
	volume = {8},
	issn = {1133-0686, 1863-8260},
	url = {http://link.springer.com/10.1007/BF02595862},
	doi = {10.1007/BF02595862},
	language = {en},
	number = {1},
	urldate = {2014-05-04TZ},
	journal = {Test},
	author = {Locantore, N. and Marron, J. S. and Simpson, D. G. and Tripoli, N. and Zhang, J. T. and Cohen, K. L. and Boente, Graciela and Fraiman, Ricardo and Brumback, Babette and Croux, Christophe and Fan, Jianqing and Kneip, Alois and Marden, John I. and Peña, Daniel and Prieto, Javier and Ramsay, Jim O. and Valderrama, Mariano J. and Aguilera, Ana M. and Locantore, N. and Marron, J. S. and Simpson, D. G. and Tripoli, N. and Zhang, J. T. and Cohen, K. L.},
	month = jun,
	year = {1999},
	pages = {1--73}
}

@article{horvath_testing_2014-1,
	title = {Testing stationarity of functional time series},
	volume = {179},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407613002327},
	doi = {10.1016/j.jeconom.2013.11.002},
	abstract = {Economic and financial data often take the form of a collection of curves observed consecutively over time. Examples include, intraday price curves, yield and term structure curves, and intraday volatility curves. Such curves can be viewed as a time series of functions. A fundamental issue that must be addressed, before an attempt is made to statistically model such data, is whether these curves, perhaps suitably transformed, form a stationary functional time series. This paper formalizes the assumption of stationarity in the context of functional time series and proposes several procedures to test the null hypothesis of stationarity. The tests are nontrivial extensions of the broadly used tests in the KPSS family. The properties of the tests under several alternatives, including change-point and I(1)I(1), are studied, and new insights, present only in the functional setting are uncovered. The theory is illustrated by a small simulation study and an application to intraday price curves.},
	number = {1},
	urldate = {2014-04-30TZ},
	journal = {Journal of Econometrics},
	author = {Horváth, Lajos and Kokoszka, Piotr and Rice, Gregory},
	month = mar,
	year = {2014},
	keywords = {Change point, Functional data, Integrated time series, Intraday price curves, Test of stationarity},
	pages = {66--82}
}

@article{hall_properties_2006,
	title = {Properties of principal component methods for functional and longitudinal data analysis},
	volume = {34},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/math/0608022},
	doi = {10.1214/009053606000000272},
	abstract = {The use of principal component methods to analyze functional data is appropriate in a wide range of different settings. In studies of ``functional data analysis,'' it has often been assumed that a sample of random functions is observed precisely, in the continuum and without noise. While this has been the traditional setting for functional data analysis, in the context of longitudinal data analysis a random function typically represents a patient, or subject, who is observed at only a small number of randomly distributed points, with nonnegligible measurement error. Nevertheless, essentially the same methods can be used in both these cases, as well as in the vast number of settings that lie between them. How is performance affected by the sampling plan? In this paper we answer that question. We show that if there is a sample of \$n\$ functions, or subjects, then estimation of eigenvalues is a semiparametric problem, with root-\$n\$ consistent estimators, even if only a few observations are made of each function, and if each observation is encumbered by noise. However, estimation of eigenfunctions becomes a nonparametric problem when observations are sparse. The optimal convergence rates in this case are those which pertain to more familiar function-estimation settings. We also describe the effects of sampling at regularly spaced points, as opposed to random points. In particular, it is shown that there are often advantages in sampling randomly. However, even in the case of noisy data there is a threshold sampling rate (depending on the number of functions treated) above which the rate of sampling (either randomly or regularly) has negligible impact on estimator performance, no matter whether eigenfunctions or eigenvectors are being estimated.},
	number = {3},
	urldate = {2014-04-29TZ},
	journal = {The Annals of Statistics},
	author = {Hall, Peter and Müller, Hans-Georg and Wang, Jane-Ling},
	month = jun,
	year = {2006},
	note = {arXiv:math/0608022},
	keywords = {62G08, 62H25 (Primary) 62M09 (Secondary), Mathematics - Statistics Theory},
	pages = {1493--1517}
}

@article{hyndman_ftsa:_2012,
	title = {ftsa: {Functional} time series analysis},
	volume = {3},
	shorttitle = {ftsa},
	url = {http://cran.r-project.org/package=ftsa},
	urldate = {2014-04-29TZ},
	journal = {R package version},
	author = {Hyndman, R. and Shang, Han Lin},
	year = {2012}
}

@article{shang_ftsa:_2013,
	title = {ftsa: {An} {R} {Package} for {Analyzing} {Functional} {Time} {Series}.},
	volume = {5},
	shorttitle = {ftsa},
	url = {http://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=20734859&AN=90616108&h=VoCjTKcRCDp0fTr4k7v2IsPSeVxEKUUUF%2FxZZ5NqOxfVay4npAT2uLy8IK47A25RBUW9z2a5BGe3T4hq%2FUVxfA%3D%3D&crl=c},
	number = {1},
	urldate = {2014-04-29TZ},
	journal = {R Journal},
	author = {Shang, Han Lin},
	year = {2013}
}

@article{shang_functional_2013,
	title = {Functional time series approach for forecasting very short-term electricity demand},
	volume = {40},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02664763.2012.740619},
	number = {1},
	urldate = {2014-04-29TZ},
	journal = {Journal of Applied Statistics},
	author = {Shang, Han Lin},
	year = {2013},
	pages = {152--168}
}

@article{hyndman_forecasting_2009,
	title = {Forecasting functional time series},
	volume = {38},
	issn = {1226-3192},
	url = {http://www.sciencedirect.com/science/article/pii/S1226319209000398},
	doi = {10.1016/j.jkss.2009.06.002},
	abstract = {We propose forecasting functional time series using weighted functional principal component regression and weighted functional partial least squares regression. These approaches allow for smooth functions, assign higher weights to more recent data, and provide a modeling scheme that is easily adapted to allow for constraints and other information. We illustrate our approaches using age-specific French female mortality rates from 1816 to 2006 and age-specific Australian fertility rates from 1921 to 2006, and show that these weighted methods improve forecast accuracy in comparison to their unweighted counterparts. We also propose two new bootstrap methods to construct prediction intervals, and evaluate and compare their empirical coverage probabilities.},
	number = {3},
	urldate = {2014-04-29TZ},
	journal = {Journal of the Korean Statistical Society},
	author = {Hyndman, Rob J. and Shang, Han Lin},
	month = sep,
	year = {2009},
	keywords = {Demographic forecasting, Functional data, Functional partial least squares, Functional principal components, Functional time series},
	pages = {199--211}
}

@article{vasicek_equilibrium_1977,
	title = {An equilibrium characterization of the term structure},
	volume = {5},
	issn = {0304-405X},
	url = {http://www.sciencedirect.com/science/article/pii/0304405X77900162},
	doi = {10.1016/0304-405X(77)90016-2},
	abstract = {The paper derives a general form of the term structure of interest rates. The following assumptions are made: (A.1) The instantaneous (spot) interest rate follows a diffusion process; (A.2) the price of a discount bond depends only on the spot rate over its term; and (A.3) the market is efficient. Under these assumptions, it is shown by means of an arbitrage argument that the expected rate of return on any bond in excess of the spot rate is proportional to its standard deviation. This property is then used to derive a partial differential equation for bond prices. The solution to that equation is given in the form of a stochastic integral representation. An interpretation of the bond pricing formula is provided. The model is illustrated on a specific case.},
	number = {2},
	journal = {Journal of Financial Economics},
	author = {Vasicek, O.},
	year = {1977},
	pages = {177 -- 188}
}

@techreport{svensson_estimating_1994,
	type = {Working {Paper}},
	title = {Estimating and {Interpreting} {Forward} {Interest} {Rates}: {Sweden} 1992 - 1994},
	url = {http://www.nber.org/papers/w4871},
	abstract = {The use of forward interest rates as a monetary policy indicator is demonstrated, using Sweden 1992-1994 as an example. The forward rates are interpreted as indicating market expectations of the time- path of future interest rates, future inflation rates, and future currency depreciation rates. They separate market expectations for the short, medium and long term more easily than the standard yield curve. Forward rates are estimated with an extended and more flexible version of Nelson and Siegel's functional form.},
	number = {4871},
	institution = {National Bureau of Economic Research},
	author = {Svensson, L.E.O.},
	month = sep,
	year = {1994}
}

@article{silverman_smoothed_1996,
	title = {Smoothed functional principal components analysis by choice of norm},
	volume = {24},
	number = {1},
	journal = {The Annals of Statistics},
	author = {Silverman, B. W.},
	year = {1996},
	pages = {1--24}
}

@article{shang_nonparametric_2011,
	title = {Nonparametric time series forecasting with dynamic updating},
	volume = {81},
	issn = {0378-4754},
	url = {http://www.sciencedirect.com/science/article/pii/S037847541000145X},
	doi = {10.1016/j.matcom.2010.04.027},
	abstract = {We present a nonparametric method to forecast a seasonal univariate time series, and propose four dynamic updating methods to improve point forecast accuracy. Our methods consider a seasonal univariate time series as a functional time series. We propose first to reduce the dimensionality by applying functional principal component analysis to the historical observations, and then to use univariate time series forecasting and functional principal component regression techniques. When data in the most recent year are partially observed, we improve point forecast accuracy by using dynamic updating methods. We also introduce a nonparametric approach to construct prediction intervals of updated forecasts, and compare the empirical coverage probability with an existing parametric method. Our approaches are data-driven and computationally fast, and hence they are feasible to be applied in real time high frequency dynamic updating. The methods are demonstrated using monthly sea surface temperatures from 1950 to 2008.},
	number = {7},
	journal = {Mathematics and Computers in Simulation},
	author = {Shang, H. L. and Hyndman, R. J.},
	year = {2011},
	note = {{\textless}ce:title{\textgreater}Selected Papers of the Combined \{IMACS\} World Congress and \{MSSANZ\} 18th Biennial Conference on Modelling and Simulation, Cairns, Australia, 13-17 July, 2009{\textless}/ce:title{\textgreater}},
	keywords = {Functional, Principal, analysis, component},
	pages = {1310 -- 1324}
}

@article{rossi_estrutura_1996,
	title = {A estrutura a termo da taxa de juros: uma síntese},
	url = {http://www.ipea.gov.br/portal/images/stories/PDFs/TDs/td_0447.pdf},
	number = {447},
	author = {Rossi, J. W.},
	year = {1996}
}

@article{nelson_parsimonious_1987,
	title = {Parsimonious modeling of yield curves},
	journal = {Journal of business},
	author = {Nelson, C. R. and Siegel, A. F.},
	year = {1987},
	pages = {473--489}
}

@article{vieu_regression_1999,
	title = {Régression non paramétrique: une approche générale du problème de sélection automatique de modèle},
	volume = {328},
	issn = {0764-4442},
	url = {http://www.sciencedirect.com/science/article/pii/S0764444299800135},
	doi = {10.1016/S0764-4442(99)80013-5},
	abstract = {This Note presents a unifying approach for data-driven model selection in nonparametric regression. Some asymptotic optimality property is presented in a general way that allows applications to many nonparametric smoothers and many model choice problems.},
	number = {1},
	journal = {Comptes Rendus de l'Académie des Sciences - Series I - Mathematics},
	author = {Vieu, P.},
	year = {1999},
	pages = {63 -- 66}
}

@article{kargin_curve_2008,
	title = {Curve forecasting by functional autoregression},
	volume = {99},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X08000961},
	doi = {10.1016/j.jmva.2008.03.001},
	abstract = {This paper deals with the prediction of curve-valued autoregression processes. It develops a novel technique, predictive factor decomposition, for the estimation of the autoregression operator. The technique is based on finding a reduced-rank approximation to the autoregression operator that minimizes the expected squared norm of the prediction error. Implementing this idea, we relate the operator approximation problem to the singular value decomposition of a combination of cross-covariance and covariance operators. We develop an estimation method based on regularization of the empirical counterpart of this singular value decomposition, prove its consistency and evaluate convergence rates. The method is illustrated by an example of the term structure of the Eurodollar futures rates. In the sample corresponding to the period of normal growth, the predictive factor technique outperforms the principal components method and performs on a par with custom-designed prediction methods.},
	number = {10},
	journal = {Journal of Multivariate Analysis},
	author = {Kargin, V. and Onatski, A.},
	year = {2008},
	keywords = {62H25},
	pages = {2508 -- 2526}
}

@article{hormann_functional_2012,
	title = {Functional time series},
	volume = {30},
	journal = {Handbook of Statistics: Time Series Analysis: Methods and Applications},
	author = {Hormann, S. and Kokoszka, P.},
	year = {2012},
	pages = {157}
}

@article{gang_functional_2009,
	title = {Functional {Time} {Series} {Prediction} {Using} {Process} {Neural} {Network}},
	volume = {26},
	url = {http://stacks.iop.org/0256-307X/26/i=9/a=090502},
	abstract = {Time series prediction methods based on conventional neural networks do not take into account the functional relations between the discrete observed values in the time series. This usually causes a low prediction accuracy. To solve this problem, a functional time series prediction model based on a process neural network is proposed in this paper. A Levenberg—Marquardt learning algorithm based on the expansion of the orthonormal basis functions is developed to train the proposed functional time series prediction model. The efficiency of the proposed functional time series prediction model and the corresponding learning algorithm is verified by the prediction of the monthly mean sunspot numbers. The comparative test results indicate that process neural network is a promising tool for functional time series prediction.},
	number = {9},
	journal = {Chinese Physics Letters},
	author = {Gang, D. and Lin, L. and Shi-Sheng, Z.},
	year = {2009},
	pages = {090502}
}

@article{freitas_estrutura_2012,
	title = {A {ESTRUTURA} {A} {TERMO} {DAS} {TAXAS} {DE} {JUROS} {NO} {BRASIL}: {TEORIA} {E} {EVIDÊNCIA} {EMPÍRICA}},
	volume = {06},
	url = {http://revistas.utfpr.edu.br/pb/index.php/CAP/article/viewFile/1558/1023},
	abstract = {Neste artigo, analisa-se o cálculo do valor de mercado de uma carteira de títulos. Usa-se para avaliar opções, swaps e contratos futuros de juros, e analisa a criação de oportunidades de arbitragem entre os títulos de renda fixa usados no mercado. Além do mais, registra-se uso na previsão das taxas de juros inscritas a títulos de longo prazo, cujos valores identificados reúnem as médias das taxas de curto prazo da economia, composta pelo grau de risco. Avalia informações sobre a trajetória futura da economia além de aplicar taxas de juros pré-fixadas.},
	number = {06},
	journal = {CAP Accounting and Management},
	author = {Freitas, R.O. and Aleixo, L. B. and Souza, W.A.R.},
	editor = {Accounting, C. A. P. and Management},
	year = {2012},
	keywords = {Juros., Mercado, Previsão, Risco, Taxas de Juros}
}

@article{ferraty_nonparametric_2004,
	title = {Nonparametric models for functional data, with application in regression, time series prediction and curve discrimination},
	volume = {16},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10485250310001622686},
	doi = {10.1080/10485250310001622686},
	abstract = {The aim of this article is to investigate a new approach for estimating a regression model with scalar response and in which the explanatory variable is valued in some abstract semi-metric functional space. Nonparametric estimates are introduced, and their behaviors are investigated in the situation of dependent data. Our study contains asymptotic results with rates. The curse of dimensionality, which is of great importance in this infinite dimensional setting, is highlighted by our asymptotic results. Some ideas, based on fractal dimension modelizations, are given to reduce dimensionality of the problem. Generalization of the model leads to possible applications in several fields of applied statistics, and we present three applications among these namely: regression estimation, time-series prediction, and curve discrimination. As a by-product of our approach in the finite-dimensional context, we give a new proof for the rates of convergence of some Nadaraya–Watson kernel-type smoother without needing any smoothness assumption on the density function of the explanatory variables.},
	number = {1-2},
	journal = {Journal of Nonparametric Statistics},
	author = {Ferraty, F. and Vieu, P.},
	year = {2004},
	pages = {111--125}
}

@book{de_boor_practical_1978,
	series = {Applied {Mathematical} {Sciences}},
	title = {A {Practical} {Guide} to {Splines}},
	isbn = {9783540903567},
	url = {http://books.google.com.br/books?id=mZMQAQAAIAAJ},
	number = {v. 27},
	publisher = {Springer-Verlag},
	author = {De Boor, C.},
	year = {1978}
}

@article{crambes_robust_2008,
	title = {Robust nonparametric estimation for functional data},
	volume = {20},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10485250802331524},
	doi = {10.1080/10485250802331524},
	abstract = {Robust estimation provides an alternative approach to classical methods, for instance, when the data are affected by the presence of outliers. Recently, these robust estimators have been considered for models with functional data. In this paper, we focus on asymptotic properties of a conditional nonparametric estimation of a real-valued variable with a functional covariate. We present results dealing with},
	number = {7},
	journal = {Journal of Nonparametric Statistics},
	author = {Crambes, C. and Delsol, L. and Laksaci, A.},
	year = {2008},
	pages = {573--598}
}

@article{benko_functional_2006,
	title = {Functional data analysis with applications in finance},
	journal = {PhD Humboldt-Universität, Berlin},
	author = {Benko, M.},
	year = {2006}
}

@article{antoniadis_bandwidth_2009,
	title = {Bandwidth selection for functional time series prediction},
	volume = {79},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/S0167715208005075},
	doi = {10.1016/j.spl.2008.10.028},
	abstract = {We propose a method to select the bandwidth for functional time series prediction. The idea underlying this method is to calculate the empirical risk of prediction using past segments of the observed series and to select as value of the bandwidth for prediction the bandwidth which minimizes this risk. We prove an oracle bound for the proposed bandwidth estimator showing that it mimics, asymptotically, the value of the bandwidth which minimizes the unknown theoretical risk of prediction based on past segments. We illustrate the usefulness of the proposed estimator in finite sample situations by means of a small simulation study and compare the resulting predictions with those obtained by a leave-one-curve-out cross-validation estimator used in the literature.},
	number = {6},
	journal = {Statistics \& Probability Letters},
	author = {Antoniadis, Anestis and Paparoditis, Efstathios and Sapatinas, Theofanis},
	year = {2009},
	pages = {733 -- 740}
}

@article{escabias_modeling_2005,
	title = {Modeling environmental data by functional principal component logistic regression},
	volume = {16},
	number = {1},
	journal = {Environmetrics},
	author = {Escabias, M. and Aguilera, A.M. and Valderrama, M.J.},
	year = {2005},
	pages = {95--107}
}

@article{diebold_forecasting_2006,
	title = {Forecasting the term structure of government bond yields},
	volume = {130},
	number = {2},
	journal = {Journal of econometrics},
	author = {Diebold, F. X. and Li, C.},
	year = {2006},
	pages = {337--364}
}

@article{delsol_advances_2009,
	title = {Advances on asymptotic normality in non-parametric functional time series analysis.},
	volume = {43},
	issn = {02331888},
	url = {http://search-ebscohost-com.ez45.periodicos.capes.gov.br/login.aspx?direct=true&db=aph&AN=36273215&lang=pt-br&site=ehost-live},
	abstract = {We consider a stationary process and wish to predict future values from previous ones. Instead of considering the process in its discretized form, we choose to see it as a sample of dependent curves. Then, we cut the process into N successive curves. Obviously, the N curves are not independent. The prediction issue can be translated into a non-parametric functional regression problem from dependent functional variables. This paper aims to revisit and complete two recent works on this topic. This article extends recent literature and provides asymptotic law with explicit constants under a-mixing assumptions. Then we establish pointwise confidence bands for the regression function. To conclude, we present how our results behave on a simulation and on a real time series. [ABSTRACT FROM AUTHOR]},
	number = {1},
	journal = {Statistics},
	author = {Delsol, L.},
	year = {2009},
	keywords = {FUNCTIONAL analysis, MATHEMATICAL statistics, REGRESSION analysis, RESEARCH, SIMULATION methods \& models, a-mixing variables, asymptotic normality, functional data, non-parametric regression, time series},
	pages = {13 -- 33}
}

@article{de_pooter_examining_2007,
	title = {Examining the {Nelson}-{Siegel} class of term structure models: {In}-sample fit versus out-of-sample forecasting performance},
	journal = {Available at SSRN 992748},
	author = {De Pooter, M.},
	year = {2007}
}

@article{dauxois_asymptotic_1982,
	title = {Asymptotic theory for the principal component analysis of a vector random function: {Some} applications to statistical inference},
	volume = {12},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/0047259X82900884},
	doi = {10.1016/0047-259X(82)90088-4},
	number = {1},
	journal = {Journal of Multivariate Analysis},
	author = {Dauxois, J. and Pousse, A. and Romain, Y.},
	year = {1982},
	keywords = {Principal, analysis, component},
	pages = {136 -- 154}
}

@article{caldeira_previsao_2011,
	title = {Previsão de {Curvas} de {Juros} {Zero}-cupom: {Estimação} {Não}-paramétrica de {Dados} {Funcionais}},
	journal = {Texto para discussão, Programa de Pós-Graduação em Economia, Universidade Federal do Rio Grande do Sul},
	author = {Caldeira, J. F. and Torrent, H.},
	year = {2011}
}

@inproceedings{caldeira_estrutura_2010,
	title = {Estrutura a {Termo} da {Taxa} de {Juros} no {Brasil}: {Observada} e {Ajustada}.},
	booktitle = {Anais do {XIII} {Encontro} {Regional} de {Economia} - {ANPEC} {SUL}. {Porto} {Alegre}},
	author = {Caldeira, J. F.},
	year = {2010}
}

@article{baillo_local_2009,
	title = {Local linear regression for functional predictor and scalar response},
	volume = {100},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X08000973},
	doi = {10.1016/j.jmva.2008.03.008},
	abstract = {The aim of this work is to introduce a new nonparametric regression technique in the context of functional covariate and scalar response. We propose a local linear regression estimator and study its asymptotic behaviour. Its finite-sample performance is compared with a Nadayara–Watson type kernel regression estimator and with the linear regression estimator via a Monte Carlo study and the analysis of two real data sets. In all the scenarios considered, the local linear regression estimator performs better than the kernel one, in the sense that the mean squared prediction error is lower.},
	number = {1},
	journal = {Journal of Multivariate Analysis},
	author = {Baíllo, Amparo and Grané, Aurea},
	year = {2009},
	keywords = {62G08},
	pages = {102 -- 111}
}

@article{barrientos-marin_locally_2010,
	title = {Locally modelled regression and functional data},
	volume = {22},
	issn = {1048-5252},
	doi = {10.1080/10485250903089930},
	abstract = {The general framework of this paper deals with the nonparametric regression of a scalar response on a functional variable (i.e. one observation can be a curve, surface, or any other object lying into an infinite-dimensional space). This paper proposes to model local behaviour of the regression operator (i.e. the link between a scalar response and an explanatory functional variable). To this end, one introduces a functional approach in the same spirit as local linear ideas in nonparametric regression. The main advantage of this functional local method is to propose an explicit expression of a kernel-type estimator which makes its computation easy and fast while keeping good predictive performance. Asymptotic properties are stated, and a functional data set illustrates the good behaviour of this functional locally modelled regression method.},
	language = {English},
	number = {5},
	journal = {Journal of Nonparametric Statistics},
	author = {Barrientos-Marin, J. and Ferraty, F. and Vieu, P.},
	year = {2010},
	keywords = {functional data, functional nonparametric statistics, locally modelled regression, rates of convergence, spectrometric curves},
	pages = {617--632}
}

@phdthesis{barrientos_marin_practical_2007,
	title = {Some practical problems of recent nonparametric procedures: testing, estimation and application},
	author = {Barrientos Marín, J.},
	year = {2007}
}

@article{aue_prediction_2012,
	title = {On the prediction of functional time series},
	url = {http://arxiv.org/pdf/1208.2892v2.pdf},
	journal = {arXiv preprint arXiv:1208.2892},
	author = {Aue, A. and Norinho, D.D. and Hörmann, S.},
	year = {2012}
}

@article{aneiros-perez_nonparametric_2008,
	title = {Nonparametric time series prediction: {A} semi-functional partial linear modeling},
	volume = {99},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X07000644},
	doi = {10.1016/j.jmva.2007.04.010},
	abstract = {There is a recent interest in developing new statistical methods to predict time series by taking into account a continuous set of past values as predictors. In this functional time series prediction approach, we propose a functional version of the partial linear model that allows both to consider additional covariates and to use a continuous path in the past to predict future values of the process. The aim of this paper is to present this model, to construct some estimates and to look at their properties both from a theoretical point of view by means of asymptotic results and from a practical perspective by treating some real data sets. Although the literature on the use of parametric or nonparametric functional modeling is growing, as far as we know, this is the first paper on semiparametric functional modeling for the prediction of time series.},
	number = {5},
	journal = {Journal of Multivariate Analysis},
	author = {Aneiros-Pérez, G. and Vieu, P.},
	year = {2008},
	keywords = {Partial, linear, regression},
	pages = {834 -- 857}
}

@article{song_futures_2012,
	title = {Futures market: contractual arrangement to restrain moral hazard in teams},
	volume = {51},
	issn = {0938-2259, 1432-0479},
	shorttitle = {Futures market},
	url = {http://link-springer-com.ez45.periodicos.capes.gov.br/article/10.1007/s00199-010-0600-8},
	doi = {10.1007/s00199-010-0600-8},
	language = {en},
	number = {1},
	urldate = {2014-03-30TZ},
	journal = {Economic Theory},
	author = {Song, Joon},
	month = sep,
	year = {2012},
	pages = {163--189}
}

@article{holmstrom_moral_1982,
	title = {Moral {Hazard} in {Teams}},
	volume = {13},
	issn = {0361915X},
	url = {http://www.jstor.org/discover/10.2307/3003457?uid=3737664&uid=379632551&uid=2&uid=3&uid=60&sid=21103580009611},
	doi = {10.2307/3003457},
	number = {2},
	urldate = {2014-03-30TZ},
	journal = {The Bell Journal of Economics},
	author = {Holmstrom, Bengt},
	year = {1982},
	pages = {324}
}

@article{engel_police_2003,
	title = {Police {Officers}' {Attitudes}, {Behavior}, and {Supervisory} {Influences}: {An} {Analysis} of {Problem} {Solving}*},
	volume = {41},
	issn = {1745-9125},
	shorttitle = {Police {Officers}' {Attitudes}, {Behavior}, and {Supervisory} {Influences}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1745-9125.2003.tb00984.x/abstract},
	doi = {10.1111/j.1745-9125.2003.tb00984.x},
	abstract = {This paper examines the influence of officers' and supervisors' attitudes and priorities toward community policing and problem solving over the time officers spend conducting problem-solving activities. Analyzing data collected for the Project on Policing Neighborhoods, a multi-method study of police patrol in two police departments, results show that officers' perceptions of their supervisors' priorities for problem solving affect the amount of time they spend conducting these activities, although their own attitudes toward community policing are unrelated to their behavior. We also find that officers' attitudes regarding problem solving are weakly correlated with their supervisors' attitudes and, further, that officers' perceptions of their supervisors' attitudes are often inaccurate.},
	language = {en},
	number = {1},
	urldate = {2014-03-30TZ},
	journal = {Criminology},
	author = {Engel, Robin Shepard and Worden, Robert E.},
	year = {2003},
	pages = {131--166}
}

@article{brehm_donut_1993,
	title = {Donut {Shops} and {Speed} {Traps}: {Evaluating} {Models} of {Supervision} on {Police} {Behavior}},
	volume = {37},
	issn = {00925853},
	shorttitle = {Donut {Shops} and {Speed} {Traps}},
	url = {http://www.jstor.org/discover/10.2307/2111384?uid=3737664&uid=379632551&uid=2&uid=3&uid=60&sid=21103580009611},
	doi = {10.2307/2111384},
	number = {2},
	urldate = {2014-03-30TZ},
	journal = {American Journal of Political Science},
	author = {Brehm, John and Gates, Scott},
	month = may,
	year = {1993},
	pages = {555}
}
@article{alchian_production_1972,
	title = {Production, {Information} {Costs}, and {Economic} {Organization}},
	volume = {62},
	issn = {0002-8282},
	language = {eng},
	number = {5},
	journal = {The American Economic Review},
	author = {Alchian, Armen A. and Demsetz, Harold},
	year = {1972},
	pages = {777--795}
}

@techreport{hellerstein_wages_1996,
	type = {Working {Paper}},
	title = {Wages, {Productivity}, and {Worker} {Characteristics}: {Evidence} from {Plant}-{Level} {Production} {Functions} and {Wage} {Equations}},
	shorttitle = {Wages, {Productivity}, and {Worker} {Characteristics}},
	url = {http://www.nber.org/papers/w5626},
	abstract = {We use a unique new data set that combines individual worker data with data on workers' employers to estimate plant-level production functions and wage equations, and thus to compare relative marginal products and relative wages for various groups of workers. The data and empirical framework lead to new evidence on numerous questions regarding the determination of wages, questions that hinge on the relationship between wages and marginal products of workers in different demographic groups. These include race and sex discrimination in wages, the causes of rising wages over the life cycle, and the returns to marriage. First, workers who have ever been married are more productive than never-married workers and are paid accordingly. Second, prime-aged workers (aged 35-54) are equally as productive as younger workers, and in some specifications are estimated to receive higher wages. However, older workers (aged 55+) are less productive than younger workers but are paid more. Third, the data indicate no difference between the relative wage and relative productivity of black workers. Finally, with the exception of managerial and professional occupations, women are paid about 25-35\% less than men, but estimated productivity differentials for women are generally no larger than 15\%, and significantly smaller than the pay differential.},
	number = {5626},
	urldate = {2014-03-29TZ},
	institution = {National Bureau of Economic Research},
	author = {Hellerstein, Judith K. and Neumark, David and Troske, Kenneth R.},
	month = jun,
	year = {1996}
}

@article{wilson_nature_1980,
	title = {The {Nature} of {Equilibrium} in {Markets} with {Adverse} {Selection}},
	volume = {11},
	issn = {0361915X},
	url = {http://www.jstor.org/discover/10.2307/3003403?uid=37618&uid=3737664&uid=5909624&uid=2&uid=37617&uid=3&uid=67&uid=62&sid=21103059187641},
	doi = {10.2307/3003403},
	number = {1},
	urldate = {2013-12-04TZ},
	journal = {The Bell Journal of Economics},
	author = {Wilson, Charles},
	year = {1980},
	pages = {108}
}

@article{akerlof_market_1970,
	title = {The market for" lemons": {Quality} uncertainty and the market mechanism},
	shorttitle = {The market for" lemons"},
	url = {http://www.jstor.org/stable/10.2307/1879431},
	urldate = {2013-12-04TZ},
	journal = {The quarterly journal of economics},
	author = {Akerlof, George A.},
	year = {1970},
	pages = {488--500}
}

@article{ades_rents_1999,
	title = {Rents, competition, and corruption},
	volume = {89},
	number = {4},
	journal = {The american economic review},
	author = {Ades, Alberto and Di Tella, Rafael},
	year = {1999},
	pages = {982--993}
}

@incollection{ehrlich_participation_1974,
	title = {Participation in illegitimate activities: {An} economic analysis},
	shorttitle = {Participation in illegitimate activities},
	url = {http://www.nber.org/chapters/c3627.pdf},
	urldate = {2013-12-05TZ},
	booktitle = {Essays in the {Economics} of {Crime} and {Punishment}},
	publisher = {UMI},
	author = {Ehrlich, Isaac},
	year = {1974},
	pages = {68--134}
}

@article{lazear_incentives_1984,
	title = {Incentives, {Productivity}, and {Labor} {Contracts}},
	volume = {99},
	issn = {0033-5533},
	abstract = {The relationship between age-earnings profiles and worker incentives is examined by contrasting wage and salary workers with the self-employed. It is argued that the steepness of wage and salary workers' age-earnings profiles reflects that desire to provide work incentives to those workers. Since self-employed workers do not face this agency problem, they are used as a benchmark to gauge productivity. Empirical support of the proposition is provided, and the effects of human capital accumulation are separated empirically from incentive effective. The most important conclusion is that under some strong assumptions, most of the slope in age-earnings profiles is accounted for by the desire to provide incentives, rather than by on-the-job training.},
	language = {eng},
	number = {2},
	journal = {The Quarterly Journal of Economics},
	author = {Lazear, Edward P. and Moore, Robert L.},
	year = {1984},
	keywords = {Business, Business ; Economics, Economics},
	pages = {275--296}
}

@article{frederiksen_incentives_2013,
	title = {Incentives and earnings growth},
	volume = {85},
	issn = {0167-2681},
	doi = {10.1016/j.jebo.2012.11.005},
	abstract = {The career prospects of newly recruited employees differ substantially within an organization. The stars experience considerable growth in earnings; others can hardly maintain their entry salaries. This article sheds light on the mechanisms generating the observed heterogeneity in earnings growth by investigating the effects that explicit short-run incentives and implicit incentives have on earnings growth. The model's predictions are tested using personnel records from a large bank and are found to be consistent with the observed earnings growth during the first half of the employees’ careers.},
	language = {eng},
	journal = {Journal of Economic Behavior and Organization},
	author = {Frederiksen, Anders},
	year = {2013},
	keywords = {Earnings Growth, Earnings Growth, Explicit Incentives, Explicit Incentives, Implicit Incentives, Implicit Incentives, J30, J30, J41, J41, M50, M50, Performance, Performance, Personnel Economics, Personnel Economics},
	pages = {97--107}
}

@article{svensson_eight_2005,
	title = {Eight {Questions} about {Corruption}},
	volume = {19},
	copyright = {Copyright © 2005 American Economic Association},
	issn = {0895-3309},
	url = {http://www.jstor.org/stable/4134971},
	number = {3},
	urldate = {2013-12-05TZ},
	journal = {The Journal of Economic Perspectives},
	author = {Svensson, Jakob},
	month = jul,
	year = {2005},
	note = {ArticleType: research-article / Full publication date: Summer, 2005 / Copyright © 2005 American Economic Association},
	pages = {19--42}
}

@book{akerlof_efficiency_1986,
	title = {Efficiency {Wage} {Models} of the {Labor} {Market}},
	isbn = {9780521312844},
	abstract = {One of the more troubling aspects of the ferment in macroeconomics that followed the demise of the Keynesian dominance in the late 1960s has been the inability of many of the new ideas to account for unemployment remains unexplained because equilibrium in most economic models occurs with supply equal to demand: if this equality holds in the labor market, there is no involuntary unemployment. Efficiency Wage Models of the Labor Market explores the reasons why there are labor market equilibria with employers preferring to pay wages in excess of the market-clearing wage and thereby explains involuntary unemployment. This volume brings together a number of the important articles on efficiency wage theory. The collection is preceded by a strong, integrative introduction, written by the editors, in which the hypothesis is set out and the variations, as described in subsequent chapters, are discussed.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Akerlof, George A. and Yellen, Janet L.},
	month = nov,
	year = {1986},
	keywords = {Business \& Economics / Econometrics, Business \& Economics / Econometrics, Business \& Economics / Economics / Macroeconomics, Business \& Economics / Economics / Macroeconomics, Business \& Economics / Economics / Theory, Business \& Economics / Economics / Theory, Business \& Economics / Labor, Business \& Economics / Labor}
}

@article{loewenstein_workers_1991,
	title = {Do workers prefer increasing wage profiles?},
	url = {http://www.jstor.org/stable/2535114},
	urldate = {2014-03-29TZ},
	journal = {Journal of Labor Economics},
	author = {Loewenstein, George and Sicherman, Nachum},
	year = {1991},
	pages = {67--84}
}

@article{becker_crime_1968,
	title = {Crime and {Punishment}: {An} {Economic} {Approach}},
	volume = {76},
	shorttitle = {Crime and {Punishment}},
	url = {http://pages.uoregon.edu/cjellis/441/Becker4.pdf},
	number = {2},
	urldate = {2013-12-05TZ},
	journal = {The Journal of Political Economy},
	author = {Becker, Gary S.},
	year = {1968},
	pages = {169--217}
}

@article{lawler_creating_2011,
	title = {Creating a new employment deal: {Total} rewards and the new workforce},
	volume = {40},
	issn = {0090-2616},
	shorttitle = {Creating a new employment deal},
	doi = {10.1016/j.orgdyn.2011.07.007},
	language = {eng},
	number = {4},
	journal = {Organizational Dynamics},
	author = {Lawler, Edward E.},
	year = {2011},
	pages = {302--309}
}

@article{hellerstein_are_1995,
	title = {Are {Earnings} {Profiles} {Steeper} {Than} {Productivity} {Profiles}? {Evidence} from {Israeli} {Firm}-{Level} {Data}},
	volume = {30},
	issn = {0022166X},
	shorttitle = {Are {Earnings} {Profiles} {Steeper} {Than} {Productivity} {Profiles}?},
	url = {http://www.jstor.org/discover/10.2307/146192?uid=3737664&uid=2134&uid=2&uid=70&uid=4&sid=21103577255121},
	doi = {10.2307/146192},
	number = {1},
	urldate = {2014-03-29TZ},
	journal = {The Journal of Human Resources},
	author = {Hellerstein, Judith K. and Neumark, David},
	year = {1995},
	pages = {89}
}

@article{ferraty_functional_2002,
	title = {Functional nonparametric model for time series: a fractal approach for dimension reduction},
	volume = {11},
	issn = {1133-0686, 1863-8260},
	shorttitle = {Functional nonparametric model for time series},
	url = {http://link.springer.com/article/10.1007/BF02595710},
	doi = {10.1007/BF02595710},
	abstract = {In this paper we propose a functional nonparametric model for time series prediction. The originality of this model consists in using as predictor a continuous set of past values. This time series problem is presented in the general framework of regression estimation from dependent samples with regressor valued in some infinite dimensional semi-normed vectorial space. The curse of dimensionality induced by our approach is overridden by means of fractal dimension considerations. We give asymptotics for a kernel type nonparametric predictor linking the rates of convergence with the fractal dimension of the functional process. Finally, our method has been implemented and applied to some electricity consumption data.},
	language = {en},
	number = {2},
	urldate = {2013-08-13TZ},
	journal = {Test},
	author = {Ferraty, Frederic and Goia, Aldo and Vieu, Philippe},
	month = dec,
	year = {2002},
	keywords = {28A80, 60G25, 62G05, 62G08, 62G20, 62G99, Fractal dimension, Functional data, Statistical Theory and Methods, Statistics for Business/Economics/Mathematical Finance/Insurance, Statistics, general, kernel estimator, mixing processes, nonparametric regression, semi-normed linear space},
	pages = {317--344}
}

@techreport{hays_functional_2012,
	type = {{arXiv} e-print},
	title = {Functional dynamic factor models with application to yield curve forecasting},
	url = {http://arxiv.org/abs/1209.6172},
	abstract = {Accurate forecasting of zero coupon bond yields for a continuum of maturities is paramount to bond portfolio management and derivative security pricing. Yet a universal model for yield curve forecasting has been elusive, and prior attempts often resulted in a trade-off between goodness of fit and consistency with economic theory. To address this, herein we propose a novel formulation which connects the dynamic factor model (DFM) framework with concepts from functional data analysis: a DFM with functional factor loading curves. This results in a model capable of forecasting functional time series. Further, in the yield curve context we show that the model retains economic interpretation. Model estimation is achieved through an expectation-maximization algorithm, where the time series parameters and factor loading curves are simultaneously estimated in a single step. Efficient computing is implemented and a data-driven smoothing parameter is nicely incorporated. We show that our model performs very well on forecasting actual yield data compared with existing approaches, especially in regard to profit-based assessment for an innovative trading exercise. We further illustrate the viability of our model to applications outside of yield forecasting.},
	number = {1209.6172},
	urldate = {2013-07-29TZ},
	author = {Hays, Spencer and Shen, Haipeng and Huang, Jianhua Z.},
	month = sep,
	year = {2012},
	note = {Annals of Applied Statistics 2012, Vol. 6, No. 3, 870-894},
	keywords = {Statistics - Applications}
}

@incollection{joreskog_structural_1994,
	address = {Hayward, CA},
	title = {Structural equation modeling with ordinal variables},
	isbn = {0-940600-35-8},
	url = {http://projecteuclid.org/euclid.lnms/1215463803},
	urldate = {2013-07-16TZ},
	booktitle = {Institute of {Mathematical} {Statistics} {Lecture} {Notes} - {Monograph} {Series}},
	publisher = {Institute of Mathematical Statistics},
	author = {Jöreskog, Karl G.},
	year = {1994},
	keywords = {Equações estruturais},
	pages = {297--310}
}

@techreport{perlin_evaluation_2009,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Evaluation of {Pairs} {Trading} {Strategy} at the {Brazilian} {Financial} {Market}},
	url = {http://papers.ssrn.com/abstract=952242},
	abstract = {Pairs trading is a popular trading strategy that tries to take advantage of market inefficiencies in order to obtain profit. The idea is simple: find two stocks that move together and take long/short positions when they diverge abnormally, hoping that the prices will converge in the future. From the academic point of view of weak market efficiency theory, pairs trading strategy shouldn’t present positive performance since, according to it, the actual price of a stock reflects its past trading data, including historical prices. This leaves us with a question, does pairs trading strategy presents positive performance for the Brazilian market? The main objective of this research is to verify the performance and risk of pairs trading in the Brazilian financial market for different frequencies of the database, daily, weekly and monthly prices for the same time period. The main conclusion of this simulation is that pairs trading strategy was a profitable and market neutral strategy at the Brazilian Market. Such profitability was consistent over a region of the strategy’s parameters. The best results were found for the highest frequency (daily), which is an intuitive result.},
	number = {ID 952242},
	urldate = {2013-06-28TZ},
	institution = {Social Science Research Network},
	author = {Perlin, Marcelo},
	month = jul,
	year = {2009},
	keywords = {Market Efficiency, Pairs Trading, Quantitative Strategy}
}

@article{harvey_forecasting_1983,
	title = {Forecasting {Economic} {Time} {Series} {With} {Structural} and {Box}-{Jenkins} {Models}: {A} {Case} {Study}},
	volume = {1},
	issn = {0735-0015, 1537-2707},
	shorttitle = {Forecasting {Economic} {Time} {Series} {With} {Structural} and {Box}-{Jenkins} {Models}},
	url = {http://amstat.tandfonline.com/doi/abs/10.1080/07350015.1983.10509355},
	doi = {10.1080/07350015.1983.10509355},
	number = {4},
	urldate = {2012-10-02TZ},
	journal = {Journal of Business \& Economic Statistics},
	author = {Harvey, A. C. and Todd, P. H. J.},
	month = oct,
	year = {1983},
	pages = {299--307}
}

@article{mcleod_diagnostic_1983,
	title = {{DIAGNOSTIC} {CHECKING} {ARMA} {TIME} {SERIES} {MODELS} {USING} {SQUARED}-{RESIDUAL} {AUTOCORRELATIONS}},
	volume = {4},
	issn = {1467-9892},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9892.1983.tb00373.x/abstract},
	doi = {10.1111/j.1467-9892.1983.tb00373.x},
	abstract = {Abstract. Squared-residual autocorrelations have been found useful in detecting nonlinear types of statistical dependence in the residuals of fitted autoregressive-moving average (ARMA) models (Granger and Andersen, 1978; Miller, 1979). In this note it is shown that the normalized squared-residual autocorrelations are asymptotically unit multivariate normal. The results of a simulation experiment confirming the small-sample validity of the proposed tests is reported.},
	language = {en},
	number = {4},
	urldate = {2012-10-02TZ},
	journal = {Journal of Time Series Analysis},
	author = {McLeod, A. I. and Li, W. K.},
	year = {1983},
	keywords = {ARMA time series, diagnostic checking, nonlinear time series, portmanteau test, testing for statistical independence},
	pages = {269--273}
}

@article{stern_econometric_1999,
	title = {Econometric analysis of global climate change},
	volume = {14},
	issn = {1364-8152},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815298000942},
	doi = {10.1016/S1364-8152(98)00094-2},
	abstract = {This paper reports on research that applies econometric time series methods to the analysis of global climate change. The aim of this research was to test hypotheses concerning the causes of the historically observed rise in global temperatures. Longer term applications include quantification of the contribution of different forcing variables to historic warming and use of the model as a module in integrated assessment. Research to date has comprised three stages. In the first stage we used the concept of Granger causality and differences between the temperature record in the northern and southern hemispheres to investigate the causes of temperature increase. In the second stage we tested various global change time series for the presence of stochastic trends. We found that most series contain a stochastic trend with the greenhouse gas series containing I(2) stochastic trends. In the third stage we developed a structural time series to investigate some of the hypotheses suggested by the earlier stages and further tested for the presence of an I(2) trend in hemispheric temperature series. We found that the two temperature series share a common I(2) stochastic trend that may have its source in radiative forcing due to greenhouse gases. There is a second non-stationary component that appears only in the northern hemisphere and appears to be related to radiative forcing due to anthropogenic sulphur emissions.},
	number = {6},
	urldate = {2012-09-30TZ},
	journal = {Environmental Modelling \& Software},
	author = {Stern, David I. and Kaufmann, Robert K.},
	month = nov,
	year = {1999},
	keywords = {Econometrics, Time series analysis, global climate change},
	pages = {597--605}
}

@book{wooldridge_introductory_2009,
	title = {Introductory {Econometrics}: {A} {Modern} {Approach}},
	isbn = {9780324581621},
	shorttitle = {Introductory {Econometrics}},
	abstract = {INTRODUCTORY ECONOMETRICS: A MODERN APPROACH, 4e illustrates how empirical researchers think about and apply econometric methods in real-world practice. The text's unique approach reflects the fact that undergraduate econometrics has moved beyond just a set of abstract tools to being genuinely useful for answering questions in business, policy evaluation, and forecasting environments. The systematic approach, which reduces clutter by introducing assumptions only as they are needed, makes absorbing the material easier and leads to better econometric practices. Its unique organization separates topics by the kinds of data being analyzed, leading to an appreciation for the important issues that arise in drawing conclusions from the different kinds of data economists use. Packed with relevant applications, INTRODUCTORY ECONOMETRICS offers a wealth of interesting data sets that can be used to reproduce the examples in the text or as the starting point for original research projects.},
	language = {en},
	publisher = {Cengage Learning},
	author = {Wooldridge, Jeffrey M.},
	year = {2009},
	keywords = {Business \& Economics / Econometrics, Business \& Economics / Economics / General, Econometrics}
}

@article{kaufmann_emissions_2006,
	title = {Emissions, {Concentrations}, \& {Temperature}: {A} {Time} {Series} {Analysis}},
	volume = {77},
	issn = {0165-0009, 1573-1480},
	shorttitle = {Emissions, {Concentrations}, \& {Temperature}},
	url = {http://www.springerlink.com/index/10.1007/s10584-006-9062-1},
	doi = {10.1007/s10584-006-9062-1},
	number = {3-4},
	urldate = {2012-10-01TZ},
	journal = {Climatic Change},
	author = {Kaufmann, Robert K. and Kauppi, Heikki and Stock, James H.},
	month = jul,
	year = {2006},
	pages = {249--278}
}

@article{frank_ensemble_2010,
	title = {Ensemble reconstruction constraints on the global carbon cycle sensitivity to climate},
	volume = {463},
	copyright = {© 2010 Nature Publishing Group},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v463/n7280/full/nature08769.html},
	doi = {10.1038/nature08769},
	abstract = {The processes controlling the carbon flux and carbon storage of the atmosphere, ocean and terrestrial biosphere are temperature sensitive and are likely to provide a positive feedback leading to amplified anthropogenic warming. Owing to this feedback, at timescales ranging from interannual to the 20–100-kyr cycles of Earth's orbital variations, warming of the climate system causes a net release of CO2 into the atmosphere; this in turn amplifies warming. But the magnitude of the climate sensitivity of the global carbon cycle (termed γ), and thus of its positive feedback strength, is under debate, giving rise to large uncertainties in global warming projections. Here we quantify the median γ as 7.7 p.p.m.v. CO2 per °C warming, with a likely range of 1.7–21.4 p.p.m.v. CO2 per °C. Sensitivity experiments exclude significant influence of pre-industrial land-use change on these estimates. Our results, based on the coupling of a probabilistic approach with an ensemble of proxy-based temperature reconstructions and pre-industrial CO2 data from three ice cores, provide robust constraints for γ on the policy-relevant multi-decadal to centennial timescales. By using an ensemble of {\textgreater}200,000 members, quantification of γ is not only improved, but also likelihoods can be assigned, thereby providing a benchmark for future model simulations. Although uncertainties do not at present allow exclusion of γ calculated from any of ten coupled carbon–climate models, we find that γ is about twice as likely to fall in the lowermost than in the uppermost quartile of their range. Our results are incompatibly lower (P {\textless} 0.05) than recent pre-industrial empirical estimates of {\textasciitilde}40 p.p.m.v. CO2 per °C (refs 6, 7), and correspondingly suggest {\textasciitilde}80\% less potential amplification of ongoing global warming.},
	language = {en},
	number = {7280},
	urldate = {2012-10-01TZ},
	journal = {Nature},
	author = {Frank, David C. and Esper, Jan and Raible, Christoph C. and Büntgen, Ulf and Trouet, Valerie and Stocker, Benjamin and Joos, Fortunat},
	month = jan,
	year = {2010},
	keywords = {DNA, Nature, RNA, astronomy, astrophysics, biochemistry, bioinformatics, biology, biotechnology, cancer, cell cycle, cell signalling, climate change, computational biology, development, developmental biology, drug discovery, earth science, ecology, environmental science, evolution, evolutionary biology, functional genomics, genetics, genomics, geophysics, immunology, interdisciplinary science, life, marine biology, materials science, medical research, medicine, metabolomics, molecular biology, molecular interactions, nanotechnology, neurobiology, neuroscience, palaeobiology, pharmacology, physics, proteomics, quantum physics, science, science news, science policy, signal transduction, structural biology, systems biology, transcriptomics},
	pages = {527--530}
}

@misc{noauthor_global_nodate,
	title = {{GLOBAL} {Land}-{Ocean} {Temperature} {Index} in 0.01 degrees {Celsius}},
	url = {http://data.giss.nasa.gov/gistemp/tabledata_v3/GLB.Ts+dSST.txt},
	urldate = {2012-09-17}
}

@article{bahn_effect_2012,
	title = {The effect of proactive adaptation on green investment},
	volume = {18},
	issn = {14629011},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1462901111001705},
	doi = {10.1016/j.envsci.2011.10.010},
	urldate = {2012-10-01TZ},
	journal = {Environmental Science \& Policy},
	author = {Bahn, O. and Chesney, M. and Gheyssens, J.},
	month = apr,
	year = {2012},
	pages = {9--24}
}

@article{fomby_tests_2003,
	title = {{TESTS} {OF} {COMMON} {DETERMINISTIC} {TREND} {SLOPES} {APPLIED} {TO} {QUARTERLY} {GLOBAL} {TEMPERATURE} {DATA}},
	volume = {17},
	issn = {0731-9053},
	url = {http://www.emeraldinsight.com/journals.htm?articleid=1759955&show=abstract},
	doi = {10.1016/S0731-9053(03)17002-8},
	urldate = {2012-10-01TZ},
	journal = {Advances in Econometrics},
	author = {Fomby, Thomas B. and Vogelsang, Timothy J.},
	month = dec,
	year = {2003},
	pages = {29--43}
}

@article{kaufmann_does_2010,
	title = {Does temperature contain a stochastic trend? {Evaluating} conflicting statistical results},
	volume = {101},
	issn = {0165-0009},
	shorttitle = {Does temperature contain a stochastic trend?},
	url = {http://www.springerlink.com.ez45.periodicos.capes.gov.br/content/vl6g4g28v8215004/abstract/},
	doi = {10.1007/s10584-009-9711-2},
	abstract = {We evaluate the claim by Gay et al. (Clim Change 94:333–349, 2009 ) that “surface temperature can be better described as a trend stationary process with a one-time permanent shock” than efforts by Kaufmann et al. (Clim Change 77:249–278, 2006 ) to model surface temperature as a time series that contains a stochastic trend that is imparted by the time series for radiative forcing. We test this claim by comparing the in-sample forecast generated by the trend stationary model with a one-time permanent shock to the in-sample forecast generated by a cointegration/error correction model that is assumed to be stable over the 1870–2000 sample period. Results indicate that the in-sample forecast generated by the cointegration/error correction model is more accurate than the in-sample forecast generated by the trend stationary model with a one-time permanent shock. Furthermore, Monte Carlo simulations of the cointegration/error correction model generate time series for temperature that are consistent with the trend-stationary-with-a-break result generated by Gay et al. (Clim Change 94:333–349, 2009 ), while the time series for radiative forcing cannot be modeled as trend stationary with a one-time shock. Based on these results, we argue that modeling surface temperature as a time series that shares a stochastic trend with radiative forcing offers the possibility of greater insights regarding the potential causes of climate change and efforts to slow its progression.},
	number = {3},
	urldate = {2012-10-01TZ},
	journal = {Climatic Change},
	author = {Kaufmann, Robert and Kauppi, Heikki and Stock, James},
	year = {2010},
	keywords = {Earth and Environmental Science},
	pages = {395--405}
}

@article{hegerl_climate_2006,
	title = {Climate sensitivity constrained by temperature reconstructions over the past seven centuries},
	volume = {440},
	issn = {0028-0836, 1476-4679},
	url = {http://www.nature.com/doifinder/10.1038/nature04679},
	doi = {10.1038/nature04679},
	number = {7087},
	urldate = {2012-10-01TZ},
	journal = {Nature},
	author = {Hegerl, Gabriele C. and Crowley, Thomas J. and Hyde, William T. and Frame, David J.},
	month = apr,
	year = {2006},
	pages = {1029--1032}
}

@article{keller_global_2008,
	title = {Global warming: a review of this mostly settled issue},
	volume = {23},
	issn = {1436-3240, 1436-3259},
	shorttitle = {Global warming},
	url = {http://www.springerlink.com/index/10.1007/s00477-008-0253-3},
	doi = {10.1007/s00477-008-0253-3},
	number = {5},
	urldate = {2012-10-01TZ},
	journal = {Stochastic Environmental Research and Risk Assessment},
	author = {Keller, Charles F.},
	month = aug,
	year = {2008},
	pages = {643--676}
}

@article{cane_progress_2006,
	title = {Progress in {Paleoclimate} {Modeling}*},
	volume = {19},
	issn = {0894-8755, 1520-0442},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/JCLI3899.1},
	doi = {10.1175/JCLI3899.1},
	number = {20},
	urldate = {2012-10-01TZ},
	journal = {Journal of Climate},
	author = {Cane, Mark A. and Braconnot, Pascale and Clement, Amy and Gildor, Hezi and Joussaume, Sylvie and Kageyama, Masa and Khodri, Myriam and Paillard, Didier and Tett, Simon and Zorita, Eduardo},
	month = oct,
	year = {2006},
	pages = {5031--5057}
}

@article{tol_greenhouse_1993,
	title = {Greenhouse statistics-time series analysis},
	volume = {48},
	issn = {0177-798X},
	url = {http://www.springerlink.com/content/j4824273j2894705/abstract/},
	doi = {10.1007/BF00864914},
	abstract = {The relationship global mean temperature — atmospheric concentration of carbon dioxide is modelled by means of time series analysis as it is used in a non-experimental statistical context. The goal is to test the hypothesis that the global mean surface air temperature rises due to the rising atmospheric concentrations of greenhouse gases.},
	number = {2},
	urldate = {2012-10-01TZ},
	journal = {Theoretical and Applied Climatology},
	author = {Tol, R. S. J. and Vos, A. F.},
	year = {1993},
	keywords = {Earth and Environmental Science},
	pages = {63--74}
}